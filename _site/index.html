<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Autumn Leaves</title>
  <link rel="stylesheet" href="/style.css" />
</head>
<body>
  <div class="content-wrapper">
    <h1 id="-autumn-leaves">üçÇ Autumn Leaves</h1>

<h3 id="autumn-allmon--developer--computational-scientist--lifelong-learner">Autumn Allmon ‚Äî Developer | Computational Scientist | Lifelong Learner</h3>

<p><em>Leafnotes from a developer exploring code, math, and music.</em></p>

<div class="window">
    <div class="window-header">
      Expectation_maximization <small>(2025-10-16)</small>
    </div>
    <div class="window-content">
      <hr />
<p>layout: post
title: ‚ÄúOn Expectation Maximization and Gaussian Mixture Models‚Äù
date: 2025-10-08
tags: [rust, machine-learning, backend, learning] ‚Äî</p>

<p><strong>Implementations</strong></p>
<ul>
  <li><a href="https://github.com/cyancirrus/stellar-math/blob/main/src/learning/kmeans.rs">KMeans</a></li>
  <li><a href="https://github.com/cyancirrus/stellar-math/blob/main/src/learning/gaussian_mixture.rs">Gaussian Mixture</a></li>
</ul>

<h3 id="a-first-glance-at-statistical-performance">A First Glance at Statistical Performance</h3>

<p><img src="./assets/kmeans_gmm_clusters.png" alt="Model Performance" /></p>

<p>Today marks an exciting milestone! I want to cover one of the final implementations in my exploration of building all core Statistical Learning techniques from scratch.</p>

<p><strong>Checklist of completed models:</strong></p>

<ul>
  <li><del>Artificial Neural Network</del></li>
  <li><del>K Nearest Neighbors</del></li>
  <li><del>Singular Value Decomposition &amp; Randomized SVD</del></li>
  <li><del>Decision Tree, Random Forest &amp; GBM</del></li>
</ul>

<p><strong>New model type unlocked:</strong><br />
<em>Expectation Maximization (EM)</em></p>

<p>The graph above shows the Gaussian Mixture Model‚Äôs fit performance with KMeans initialization. In this article, I‚Äôll go through implementing Expectation Maximization from scratch, and discuss some statistical and programmatic considerations.</p>

<p>To reproduce the example:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git clone https://github.com/cyancirrus/stellar-math
cargo run <span class="nt">--example</span> gmm
open kmeans_gmm_clusters.png
</code></pre></div></div>

<h3 id="introduction">Introduction</h3>

<p>We‚Äôll first cover the primitive types used in this implementation: KMeans and Gaussian Mixtures. Both are closely related, though conceptually distinct.</p>

<p><strong>Gaussian Distribution Recap:</strong>
The Gaussian distribution, also known as the Normal distribution, is ubiquitous in statistics and named after mathematician Carl Friedrich Gauss. Formally:</p>

<p>$\text{Gaussian} = \text{Normal} :: N(\mu, \sigma^2)$</p>

<p><strong>KMeans vs GMM:</strong></p>

<ul>
  <li><strong>KMeans:</strong> Fits (k) Gaussian distributions with identical variance. Each data point receives a <em>hard</em> assignment.</li>
  <li><strong>GMM:</strong> Fits (k) Gaussian distributions with potentially different variances. Each data point receives a <em>soft</em> assignment.</li>
</ul>

<p>Both methods are widely used to cluster data into distinct segments.</p>

<h3 id="preliminaries--fitting-kmeans">Preliminaries ‚Äî Fitting KMeans</h3>

<p><strong>Known properties of KMeans:</strong></p>

<ul>
  <li>Equal variance</li>
  <li>Deterministic assignments</li>
  <li>Mixture parameters (centroids)</li>
</ul>

<p><strong>Algorithm steps:</strong></p>

<ol>
  <li>Randomly initialize cluster means and assume equal probability for each centroid.</li>
  <li>Assign each point to the closest cluster based on Euclidean distance.</li>
  <li>Update cluster means.</li>
  <li>Iterate until convergence (when mean updates are small).</li>
</ol>

<p><strong>Gaussian distance simplification:</strong></p>

<p>$ d^*(x; \mu) = (x - \mu)‚Äô(x - \mu)$</p>

<p>This is equivalent to the squared Euclidean distance to the cluster centroid. The computational cost of KMeans:</p>

<p>$O(n \cdot d \cdot k)$</p>

<p>An optimization trick:</p>

<p>$(x-\mu)‚Äô(x-\mu) = \langle x,x \rangle + \langle \mu,\mu \rangle - 2 \langle x,\mu \rangle$</p>

<h3 id="expectation-maximization-for-gmms">Expectation Maximization for GMMs</h3>

<p>GMMs introduce probabilistic assignments. The probability that a point belongs to cluster (k) is called the <em>mixing parameter</em>:</p>

<p>$pi_k(x) = \frac{\gamma_k(x)}{\sum_j \gamma_j(x)}$</p>

<p>The total probability of a data point:</p>

<p>$P(x) = \sum_k \pi_k(x) f_k(x; \mu_k, \Sigma_k)$</p>

<p>where (f_k) is the Gaussian PDF with potentially distinct covariance (\Sigma_k).</p>

<p><strong>EM steps:</strong></p>

<ol>
  <li>
    <p><strong>Expectation (E-step):</strong></p>

    <ul>
      <li>Use current parameters to compute cluster probabilities for each data point.</li>
      <li>Normalize probabilities so they sum to 1.</li>
    </ul>
  </li>
  <li>
    <p><strong>Maximization (M-step):</strong></p>

    <ul>
      <li>Update (\mu_k) and (\Sigma_k) for each cluster based on weighted averages.</li>
      <li>Rescale mixing parameters.</li>
    </ul>
  </li>
</ol>

<p>Iterate until convergence.</p>

<h3 id="statistical-considerations">Statistical Considerations</h3>

<p><strong>Probability scaling and floating-point stability:</strong>
Naively computing probabilities can lead to underflows. Using log probabilities helps:</p>

<p>$ \ln f(x; \mu, \Sigma) = -\frac{d}{2}\ln(2\pi) - \frac{1}{2}\ln\vert \Sigma \vert - \frac{1}{2} (x-\mu)‚Äô\Sigma^{-1}(x-\mu) $</p>

<p>Rescale using the maximum log probability to prevent negative infinity:</p>

<p>$l^*(x) = l(x) - \max_k l(x;\text{cluster}_k)$</p>

<div class="language-rust highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">let</span> <span class="k">mut</span> <span class="n">max_ln_prob</span> <span class="o">=</span> <span class="nn">f32</span><span class="p">::</span><span class="n">MIN</span><span class="p">;</span>
<span class="k">for</span> <span class="n">k</span> <span class="k">in</span> <span class="mi">0</span><span class="o">..</span><span class="k">self</span><span class="py">.centroids</span> <span class="p">{</span>
    <span class="k">for</span> <span class="n">c</span> <span class="k">in</span> <span class="mi">0</span><span class="o">..</span><span class="k">self</span><span class="py">.cardinality</span> <span class="p">{</span>
        <span class="k">let</span> <span class="n">val</span> <span class="o">=</span> <span class="n">x_i</span><span class="p">[</span><span class="n">c</span><span class="p">]</span> <span class="o">-</span> <span class="k">self</span><span class="py">.means</span><span class="p">[</span><span class="n">k</span><span class="p">][</span><span class="n">c</span><span class="p">];</span>
        <span class="n">x_bar</span><span class="p">[</span><span class="n">c</span><span class="p">]</span> <span class="o">=</span> <span class="n">val</span><span class="p">;</span>
        <span class="n">z_buf</span><span class="p">[</span><span class="n">c</span><span class="p">]</span> <span class="o">=</span> <span class="n">val</span><span class="p">;</span>
    <span class="p">}</span>
    <span class="n">probs</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="k">self</span><span class="py">.mixtures</span><span class="p">[</span><span class="n">k</span><span class="p">]</span><span class="nf">.ln</span><span class="p">()</span> <span class="o">+</span> <span class="nf">ln_gaussian</span><span class="p">(</span><span class="o">&amp;</span><span class="k">mut</span> <span class="n">x_bar</span><span class="p">,</span> <span class="o">&amp;</span><span class="k">mut</span> <span class="n">z_buf</span><span class="p">,</span> <span class="n">dets</span><span class="p">[</span><span class="n">k</span><span class="p">],</span> <span class="o">&amp;</span><span class="n">lus</span><span class="p">[</span><span class="n">k</span><span class="p">]);</span>
    <span class="n">max_ln_prob</span>  <span class="o">=</span> <span class="n">max_ln_prob</span><span class="nf">.max</span><span class="p">(</span><span class="n">probs</span><span class="p">[</span><span class="n">k</span><span class="p">]);</span>
<span class="p">}</span>
<span class="k">let</span> <span class="k">mut</span> <span class="n">scaler</span> <span class="o">=</span> <span class="n">EPSILON</span><span class="p">;</span>
<span class="k">for</span> <span class="n">k</span> <span class="k">in</span> <span class="mi">0</span><span class="o">..</span><span class="k">self</span><span class="py">.centroids</span> <span class="p">{</span>
    <span class="n">probs</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">probs</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">-</span> <span class="n">max_ln_prob</span><span class="p">)</span><span class="nf">.exp</span><span class="p">()</span><span class="nf">.max</span><span class="p">(</span><span class="n">EPSILON</span><span class="p">);</span>
    <span class="n">scaler</span> <span class="o">+=</span> <span class="n">probs</span><span class="p">[</span><span class="n">k</span><span class="p">];</span>
<span class="p">}</span>
</code></pre></div></div>

<p><strong>Covariance matrix computation:</strong>
Use LU or Cholesky decomposition to solve $(x-\mu)‚Äô\Sigma^{-1}(x-\mu)$ efficiently.</p>

<div class="language-rust highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">fn</span> <span class="nf">ln_gaussian</span><span class="p">(</span><span class="n">x_bar</span><span class="p">:</span><span class="o">&amp;</span><span class="k">mut</span> <span class="nb">Vec</span><span class="o">&lt;</span><span class="nb">f32</span><span class="o">&gt;</span><span class="p">,</span> <span class="n">z_buf</span><span class="p">:</span><span class="o">&amp;</span><span class="k">mut</span> <span class="nb">Vec</span><span class="o">&lt;</span><span class="nb">f32</span><span class="o">&gt;</span><span class="p">,</span> <span class="n">det</span><span class="p">:</span><span class="nb">f32</span><span class="p">,</span> <span class="n">lu</span><span class="p">:</span><span class="o">&amp;</span><span class="n">LuDecomposition</span><span class="p">)</span> <span class="k">-&gt;</span> <span class="nb">f32</span> <span class="p">{</span>
    <span class="nd">debug_assert_eq!</span><span class="p">(</span><span class="n">x_bar</span><span class="nf">.to_vec</span><span class="p">(),</span> <span class="n">z_buf</span><span class="nf">.to_vec</span><span class="p">());</span>
    <span class="k">let</span> <span class="n">card</span> <span class="o">=</span> <span class="n">x_bar</span><span class="nf">.len</span><span class="p">();</span>
    <span class="n">lu</span><span class="nf">.solve_inplace_vec</span><span class="p">(</span><span class="n">z_buf</span><span class="p">);</span>
    <span class="k">let</span> <span class="n">scaling</span> <span class="o">=</span> <span class="nf">dot_product</span><span class="p">(</span><span class="o">&amp;</span><span class="n">z_buf</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">x_bar</span><span class="p">)</span> <span class="o">/</span> <span class="mf">2_f32</span><span class="p">;</span>
    <span class="o">-</span><span class="p">(</span><span class="n">card</span> <span class="k">as</span> <span class="nb">f32</span> <span class="o">/</span> <span class="mf">2f32</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="mf">2f32</span> <span class="o">*</span> <span class="nn">std</span><span class="p">::</span><span class="nn">f32</span><span class="p">::</span><span class="nn">consts</span><span class="p">::</span><span class="n">PI</span><span class="p">)</span><span class="nf">.ln</span><span class="p">()</span>
    <span class="o">-</span> <span class="mf">0.5f32</span> <span class="o">*</span> <span class="n">det</span><span class="nf">.ln</span><span class="p">()</span>
    <span class="o">-</span> <span class="n">scaling</span>
<span class="p">}</span>
</code></pre></div></div>

<h3 id="implementation-overview">Implementation Overview</h3>

<p>EM can be implemented efficiently by computing soft assignments and immediately updating centroids:</p>

<div class="language-rust highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">pub</span> <span class="k">fn</span> <span class="nf">expectation_maximization</span><span class="p">(</span><span class="o">&amp;</span><span class="k">mut</span> <span class="k">self</span><span class="p">,</span> <span class="n">data</span><span class="p">:</span><span class="o">&amp;</span><span class="p">[</span><span class="nb">Vec</span><span class="o">&lt;</span><span class="nb">f32</span><span class="o">&gt;</span><span class="p">])</span> <span class="p">{</span>
    <span class="k">let</span> <span class="k">mut</span> <span class="n">sum_linear</span> <span class="o">=</span> <span class="nd">vec!</span><span class="p">[</span><span class="nd">vec!</span><span class="p">[</span><span class="mf">0_f32</span><span class="p">;</span> <span class="k">self</span><span class="py">.cardinality</span><span class="p">];</span> <span class="k">self</span><span class="py">.centroids</span><span class="p">];</span>
    <span class="k">let</span> <span class="k">mut</span> <span class="n">sum_squares</span> <span class="o">=</span> <span class="nd">vec!</span><span class="p">[</span><span class="nf">generate_zero_matrix</span><span class="p">(</span><span class="k">self</span><span class="py">.cardinality</span><span class="p">,</span> <span class="k">self</span><span class="py">.cardinality</span><span class="p">);</span> <span class="k">self</span><span class="py">.centroids</span><span class="p">];</span>
    <span class="o">...</span>
<span class="p">}</span>
</code></pre></div></div>

<h3 id="recap-and-takeaways">Recap and Takeaways</h3>

<ul>
  <li>EM elegantly combines statistical reasoning with numerical programming tricks such as triangular solves, computational reuse, and log stabilization.</li>
  <li>Initializing GMMs with KMeans centroids helps convergence.</li>
  <li>With this, the basic Statistical Learning algorithms are fully implemented in Rust from scratch!</li>
</ul>

<p>I think my next steps will be to explore some Control Theory in C++ or perhaps extending Fourier transforms to arbitrary dimensions.</p>

<p>Keep coding!</p>

    </div>
  </div>

<div class="window">
    <div class="window-header">
      Implementation of Tree based Models <small>(2025-10-08)</small>
    </div>
    <div class="window-content">
      <p><strong>Implementations</strong></p>
<ul>
  <li><a href="https://github.com/cyancirrus/stellar-math/blob/main/src/learning/gradient_boost.rs">Gradient Boost</a></li>
  <li><a href="https://github.com/cyancirrus/stellar-math/blob/main/src/learning/random_forest.rs">Random Forest</a></li>
  <li><a href="https://github.com/cyancirrus/stellar-math/blob/main/src/learning/decision_tree.rs">Decision Tree</a></li>
</ul>

<h3 id="a-first-glance-of-the-statistical-performance">A First Glance of the Statistical Performance</h3>

<p><img src="./assets/tree_based_models_total_variance_explained.png" alt="Model Performance" /></p>

<p>The above chart is drawn from the <strong>Boston Housing dataset</strong>, which predicts the median value of houses from a handful of features. The data is small and noisy ‚Äî which is <em>actually perfect</em> for testing robustness.</p>

<p>Each model was trained 36 times. The measure used is <strong>Total Variance Explained (TVE)</strong>:</p>

<p>Total Variance Explained (TVE), Sum Squares Error (SSE) are defined as</p>

<blockquote>
  <p>$SSE(Model) := Sum (y_i - \hat{y_i})^2$</p>
</blockquote>

<blockquote>
  <p>$SSE(Data)  := Sum (y_i - mean)^2$</p>
</blockquote>

<blockquote>
  <p>$TotalVarianceExplained := 1 - \frac{SSE(model)}{SSE(Data)}$</p>
</blockquote>

<p><em>All TVE metrics below refer to unseen test data.</em></p>

<ul>
  <li>
    <p><strong>Random Forest ‚Äî Average TVE ~78%</strong><br />
Consistently strong across runs. The ensemble effect helps debias training data and generalize well.</p>
  </li>
  <li>
    <p><strong>Gradient Boosting ‚Äî Average TVE ~62%</strong><br />
Overfits heavily due to small sample size. Gradient boosting shines with large, diverse datasets. Here, it‚Äôs unstable ‚Äî high variance between best and worst runs.</p>
  </li>
  <li>
    <p><strong>Decision Tree ‚Äî Average TVE ~67%</strong><br />
Transparent, interpretable, slightly overfits but remains solid. You can inspect node-by-node which features drive predictions.</p>
  </li>
</ul>

<p>To reproduce:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git clone https://github.com/cyancirrus/stellar-math
cargo run <span class="nt">--example</span> trees
open tve_chart.png
</code></pre></div></div>

<h3 id="introduction">Introduction</h3>

<p>Machine learning models often emerge from a few core structures ‚Äî perceptrons, decision trees, EM, clustering, and dimensionality reduction.
Here, we‚Äôll explore how to implement three of the most classic ones:</p>

<ul>
  <li>Decision Trees</li>
  <li>Random Forests</li>
  <li>Gradient Boosting</li>
</ul>

<h3 id="preliminaries--what-is-a-decision-tree">Preliminaries ‚Äî What <em>is</em> a Decision Tree?</h3>

<p>Let‚Äôs start simple: a humble <code class="language-plaintext highlighter-rouge">if</code> / <code class="language-plaintext highlighter-rouge">else</code> statement.</p>

<div class="language-rust highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">if</span> <span class="n">it_is_raining</span> <span class="p">{</span>
    <span class="n">items_to_carry</span> <span class="o">+=</span> <span class="s">"umbrella"</span><span class="p">;</span>
<span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
    <span class="c1">// no umbrella needed</span>
<span class="p">}</span>
</code></pre></div></div>

<p>That‚Äôs a one-node decision tree.
Now, imagine scaling that intuition up ‚Äî predicting <em>wine quality</em>:</p>

<p>Features might include:</p>

<ul>
  <li>country of origin</li>
  <li>year of production</li>
  <li>rainfall, temperature</li>
  <li>brand</li>
  <li>colour (white/red/red-blend)</li>
  <li>type (merlot, pinot, etc.)</li>
  <li>price</li>
  <li><strong>quality</strong> ‚Üê target variable</li>
</ul>

<div class="language-rust highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">if</span> <span class="n">country</span> <span class="o">==</span> <span class="s">"greece"</span> <span class="o">&amp;&amp;</span> <span class="n">colour</span> <span class="o">==</span> <span class="s">"green"</span> <span class="p">{</span>
    <span class="n">predicted_quality</span> <span class="o">=</span> <span class="mf">1000_f32</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div>

<p>That‚Äôs the backbone idea: a hierarchy of <code class="language-plaintext highlighter-rouge">if-else</code> splits, each narrowing down the prediction.</p>

<p><em>Algorithmically</em></p>

<div class="language-rust highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">let</span> <span class="k">mut</span> <span class="n">best_split</span> <span class="o">=</span> <span class="nb">None</span><span class="p">;</span>
<span class="k">let</span> <span class="k">mut</span> <span class="n">explanatory_power</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">;</span>

<span class="k">for</span> <span class="n">_</span> <span class="k">in</span> <span class="mi">0</span><span class="o">..</span><span class="n">desired_number_of_nodes</span> <span class="p">{</span>
    <span class="k">for</span> <span class="n">each_dimension</span> <span class="k">in</span> <span class="n">data</span> <span class="p">{</span>
        <span class="k">if</span> <span class="nf">explanatory_power</span><span class="p">(</span><span class="n">this_split</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">explanatory_power</span> <span class="p">{</span>
            <span class="n">best_split</span> <span class="o">=</span> <span class="n">this_split</span><span class="p">;</span>
            <span class="n">explanatory_power</span> <span class="o">=</span> <span class="nf">explanatory_power</span><span class="p">(</span><span class="n">this_split</span><span class="p">);</span>
        <span class="p">}</span>
    <span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div>

<p>Each node only handles its subset of data ‚Äî not the entire dataset ‚Äî as we recursively partition.</p>

<h3 id="decision-tree-extensions">Decision Tree Extensions</h3>

<h4 id="random-forest">Random Forest</h4>

<p>A <strong>Random Forest</strong> is just a collection of decision trees ‚Äî each trained on a subsample of data or features.</p>

<div class="language-rust highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">let</span> <span class="k">mut</span> <span class="n">prediction</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">;</span>
<span class="k">for</span> <span class="n">tree</span> <span class="k">in</span> <span class="n">forest</span> <span class="p">{</span>
   <span class="n">prediction</span> <span class="o">+=</span> <span class="n">tree</span><span class="nf">.predict</span><span class="p">(</span><span class="n">unseen_data</span><span class="p">);</span>
<span class="p">}</span>
<span class="n">prediction</span> <span class="o">/=</span> <span class="n">forest</span><span class="nf">.len</span><span class="p">()</span> <span class="k">as</span> <span class="nb">f32</span><span class="p">;</span>
</code></pre></div></div>

<p>This ensemble effect dramatically reduces variance and overfitting.</p>

<h4 id="gradient-boosting">Gradient Boosting</h4>

<p>Instead of averaging, <strong>Gradient Boosting</strong> fits each new tree to the <em>residual errors</em> of the previous one.
It‚Äôs like an iterative ‚Äúerror correction‚Äù process:</p>

<p>$prediction = y_0 + (\hat{y_0} - y_1) + (\hat{y_1} - y_2) + ‚Ä¶ + (\hat{y_{n-1}} - y_n)$</p>

<p>Each new model learns to predict what its predecessors <em>missed</em>.</p>

<h3 id="elphabas-look-at-the-great-wizard-of-oz--tree-extensions--implementations">Elphaba‚Äôs Look at the Great Wizard of Oz ‚Äî Tree Extensions &amp; Implementations</h3>

<p>Everyone wants to peek behind the curtain at the ‚Äúcomplex‚Äù methods ‚Äî
and then realize they‚Äôre surprisingly elegant.</p>

<h4 id="gradient-boosting-rust">Gradient Boosting (Rust)</h4>

<details>
<summary>Click to expand code</summary>


<figure class="highlight"><pre><code class="language-rust" data-lang="rust"><span class="k">use</span> <span class="k">crate</span><span class="p">::</span><span class="nn">learning</span><span class="p">::</span><span class="nn">decision_tree</span><span class="p">::{</span><span class="n">DecisionTree</span><span class="p">,</span> <span class="n">DecisionTreeModel</span><span class="p">};</span>

<span class="k">pub</span> <span class="k">struct</span> <span class="n">GradientBoost</span> <span class="p">{</span>
    <span class="n">trees</span><span class="p">:</span> <span class="nb">usize</span><span class="p">,</span>
    <span class="n">forest</span><span class="p">:</span> <span class="nb">Vec</span><span class="o">&lt;</span><span class="n">DecisionTreeModel</span><span class="o">&gt;</span><span class="p">,</span>
<span class="p">}</span>

<span class="k">impl</span> <span class="n">GradientBoost</span> <span class="p">{</span>
    <span class="k">pub</span> <span class="k">fn</span> <span class="nf">new</span><span class="p">(</span>
        <span class="n">data</span><span class="p">:</span> <span class="o">&amp;</span><span class="k">mut</span> <span class="nb">Vec</span><span class="o">&lt;</span><span class="nb">Vec</span><span class="o">&lt;</span><span class="nb">f32</span><span class="o">&gt;&gt;</span><span class="p">,</span>
        <span class="n">trees</span><span class="p">:</span> <span class="nb">usize</span><span class="p">,</span>
        <span class="n">nodes</span><span class="p">:</span> <span class="nb">usize</span><span class="p">,</span>
        <span class="n">obs_sample</span><span class="p">:</span> <span class="nb">f32</span><span class="p">,</span>
        <span class="n">dim_sample</span><span class="p">:</span> <span class="nb">f32</span>
    <span class="p">)</span> <span class="k">-&gt;</span> <span class="k">Self</span> <span class="p">{</span>
        <span class="k">if</span> <span class="n">data</span><span class="nf">.is_empty</span><span class="p">()</span> <span class="p">||</span> <span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="nf">.is_empty</span><span class="p">()</span> <span class="p">{</span>
            <span class="nd">panic!</span><span class="p">(</span><span class="s">"data is empty"</span><span class="p">);</span>
        <span class="p">}</span>
        <span class="k">let</span> <span class="n">n_obs</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="nf">.len</span><span class="p">();</span>
        <span class="k">let</span> <span class="n">dims</span> <span class="o">=</span> <span class="n">data</span><span class="nf">.len</span><span class="p">();</span>
        <span class="k">let</span> <span class="n">target_idx</span> <span class="o">=</span> <span class="n">data</span><span class="nf">.len</span><span class="p">()</span> <span class="o">-</span> <span class="mi">1</span><span class="p">;</span>
        <span class="k">let</span> <span class="k">mut</span> <span class="n">sample</span> <span class="o">=</span> <span class="nd">vec!</span><span class="p">[</span><span class="mf">0_f32</span><span class="p">;</span> <span class="n">dims</span><span class="p">];</span>
        <span class="k">let</span> <span class="k">mut</span> <span class="n">forest</span> <span class="o">=</span> <span class="nn">Vec</span><span class="p">::</span><span class="nf">with_capacity</span><span class="p">(</span><span class="n">trees</span><span class="p">);</span>

        <span class="k">for</span> <span class="n">_</span> <span class="k">in</span> <span class="mi">0</span><span class="o">..</span><span class="n">trees</span> <span class="p">{</span>
            <span class="k">let</span> <span class="n">tree</span> <span class="o">=</span> <span class="nn">DecisionTree</span><span class="p">::</span><span class="nf">new</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">obs_sample</span><span class="p">,</span> <span class="n">dim_sample</span><span class="p">)</span><span class="nf">.train</span><span class="p">(</span><span class="n">nodes</span><span class="p">);</span>
            <span class="k">for</span> <span class="n">idx</span> <span class="k">in</span> <span class="mi">0</span><span class="o">..</span><span class="n">n_obs</span> <span class="p">{</span>
                <span class="k">for</span> <span class="n">d</span> <span class="k">in</span> <span class="mi">0</span><span class="o">..</span><span class="n">dims</span> <span class="p">{</span> <span class="n">sample</span><span class="p">[</span><span class="n">d</span><span class="p">]</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">d</span><span class="p">][</span><span class="n">idx</span><span class="p">];</span> <span class="p">}</span>
                <span class="k">let</span> <span class="n">pred</span> <span class="o">=</span> <span class="n">tree</span><span class="nf">.predict</span><span class="p">(</span><span class="o">&amp;</span><span class="n">sample</span><span class="p">);</span>
                <span class="n">data</span><span class="p">[</span><span class="n">target_idx</span><span class="p">][</span><span class="n">idx</span><span class="p">]</span> <span class="o">-=</span> <span class="n">pred</span><span class="p">;</span>
            <span class="p">}</span>
            <span class="n">forest</span><span class="nf">.push</span><span class="p">(</span><span class="n">tree</span><span class="p">);</span>
        <span class="p">}</span>
        <span class="k">Self</span> <span class="p">{</span> <span class="n">trees</span><span class="p">,</span> <span class="n">forest</span> <span class="p">}</span>
    <span class="p">}</span>

    <span class="k">pub</span> <span class="k">fn</span> <span class="nf">predict</span><span class="p">(</span><span class="o">&amp;</span><span class="k">self</span><span class="p">,</span> <span class="n">data</span><span class="p">:</span> <span class="o">&amp;</span><span class="p">[</span><span class="nb">f32</span><span class="p">])</span> <span class="k">-&gt;</span> <span class="nb">f32</span> <span class="p">{</span>
        <span class="k">self</span><span class="py">.forest</span><span class="nf">.iter</span><span class="p">()</span><span class="nf">.map</span><span class="p">(|</span><span class="n">t</span><span class="p">|</span> <span class="n">t</span><span class="nf">.predict</span><span class="p">(</span><span class="n">data</span><span class="p">))</span><span class="nf">.sum</span><span class="p">()</span>
    <span class="p">}</span>
<span class="p">}</span></code></pre></figure>

</details>

<h4 id="random-forest-rust">Random Forest (Rust)</h4>

<details>
<summary>Click to expand code</summary>

<figure class="highlight"><pre><code class="language-rust" data-lang="rust"><span class="k">use</span> <span class="k">crate</span><span class="p">::</span><span class="nn">learning</span><span class="p">::</span><span class="nn">decision_tree</span><span class="p">::{</span><span class="n">DecisionTree</span><span class="p">,</span> <span class="n">DecisionTreeModel</span><span class="p">};</span>

<span class="k">pub</span> <span class="k">struct</span> <span class="n">RandomForest</span> <span class="p">{</span>
    <span class="n">trees</span><span class="p">:</span> <span class="nb">usize</span><span class="p">,</span>
    <span class="n">forest</span><span class="p">:</span> <span class="nb">Vec</span><span class="o">&lt;</span><span class="n">DecisionTreeModel</span><span class="o">&gt;</span><span class="p">,</span>
<span class="p">}</span>

<span class="k">impl</span> <span class="n">RandomForest</span> <span class="p">{</span>
    <span class="k">pub</span> <span class="k">fn</span> <span class="nf">new</span><span class="p">(</span>
        <span class="n">data</span><span class="p">:</span> <span class="o">&amp;</span><span class="nb">Vec</span><span class="o">&lt;</span><span class="nb">Vec</span><span class="o">&lt;</span><span class="nb">f32</span><span class="o">&gt;&gt;</span><span class="p">,</span>
        <span class="n">trees</span><span class="p">:</span> <span class="nb">usize</span><span class="p">,</span>
        <span class="n">nodes</span><span class="p">:</span> <span class="nb">usize</span><span class="p">,</span>
        <span class="n">obs_sample</span><span class="p">:</span> <span class="nb">f32</span><span class="p">,</span>
        <span class="n">dim_sample</span><span class="p">:</span> <span class="nb">f32</span>
    <span class="p">)</span> <span class="k">-&gt;</span> <span class="k">Self</span> <span class="p">{</span>
        <span class="k">let</span> <span class="n">forest</span> <span class="o">=</span> <span class="p">(</span><span class="mi">0</span><span class="o">..</span><span class="n">trees</span><span class="p">)</span>
            <span class="nf">.map</span><span class="p">(|</span><span class="n">_</span><span class="p">|</span> <span class="p">{</span>
                <span class="k">let</span> <span class="k">mut</span> <span class="n">tree</span> <span class="o">=</span> <span class="nn">DecisionTree</span><span class="p">::</span><span class="nf">new</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">obs_sample</span><span class="p">,</span> <span class="n">dim_sample</span><span class="p">);</span>
                <span class="n">tree</span><span class="nf">.train</span><span class="p">(</span><span class="n">nodes</span><span class="p">)</span>
            <span class="p">})</span>
            <span class="nf">.collect</span><span class="p">();</span>
        <span class="k">Self</span> <span class="p">{</span> <span class="n">trees</span><span class="p">,</span> <span class="n">forest</span> <span class="p">}</span>
    <span class="p">}</span>

    <span class="k">pub</span> <span class="k">fn</span> <span class="nf">predict</span><span class="p">(</span><span class="o">&amp;</span><span class="k">self</span><span class="p">,</span> <span class="n">data</span><span class="p">:</span> <span class="o">&amp;</span><span class="p">[</span><span class="nb">f32</span><span class="p">])</span> <span class="k">-&gt;</span> <span class="nb">f32</span> <span class="p">{</span>
        <span class="k">self</span><span class="py">.forest</span><span class="nf">.iter</span><span class="p">()</span><span class="nf">.map</span><span class="p">(|</span><span class="n">t</span><span class="p">|</span> <span class="n">t</span><span class="nf">.predict</span><span class="p">(</span><span class="n">data</span><span class="p">))</span><span class="py">.sum</span><span class="p">::</span><span class="o">&lt;</span><span class="nb">f32</span><span class="o">&gt;</span><span class="p">()</span> <span class="o">/</span> <span class="k">self</span><span class="py">.trees</span> <span class="k">as</span> <span class="nb">f32</span>
    <span class="p">}</span>
<span class="p">}</span></code></pre></figure>

</details>

<p>See? Behind the magic ‚Äî they‚Äôre just <em>wrappers</em> around the core decision tree.</p>

<h3 id="decision-tree-implementation--complexity">Decision Tree Implementation &amp; Complexity</h3>

<p>For each tree, we scan all possible splits.
Each dimension‚Äôs data is pre-sorted for efficiency.</p>

<p><strong>Initial sort cost:</strong>
$O(d \cdot n \log n)$</p>

<p><strong>Split evaluation per node:</strong></p>

<p>$ BaseNodeSSE = SumSquares - \frac{\text{sum_linear}^2}{\text{cardinality}} $
$ SplitNodeSSE = SSE(left) + SSE(right)$</p>

<p>Since we can compute these incrementally as we scan, the cost per split is linear.</p>

<p><strong>Overall cost:</strong></p>

<p>$O(d \cdot n \log n + s \cdot n \cdot d)$<br />
$\approx O(d \cdot n \log n)$</p>

<p>That‚Äôs the <em>pi√®ce de r√©sistance</em>:
the whole algorithm‚Äôs cost is roughly that of the initial sort!</p>

<details>
<summary>Click to expand code</summary>

<figure class="highlight"><pre><code class="language-rust" data-lang="rust"><span class="k">fn</span> <span class="nf">delta</span><span class="p">(</span><span class="o">&amp;</span><span class="k">self</span><span class="p">,</span> <span class="n">running</span><span class="p">:</span> <span class="o">&amp;</span><span class="k">Self</span><span class="p">)</span> <span class="k">-&gt;</span> <span class="nb">f32</span> <span class="p">{</span>
    <span class="k">if</span> <span class="k">self</span><span class="py">.card</span> <span class="o">==</span> <span class="mi">0</span> <span class="p">||</span> <span class="n">running</span><span class="py">.card</span> <span class="o">==</span> <span class="mi">0</span> <span class="p">||</span> <span class="k">self</span><span class="py">.card</span>  <span class="o">==</span> <span class="n">running</span><span class="py">.card</span> <span class="p">{</span> <span class="k">return</span> <span class="mf">0_f32</span> <span class="p">};</span>
    <span class="k">let</span> <span class="p">(</span><span class="n">card</span><span class="p">,</span> <span class="n">l_card</span><span class="p">,</span> <span class="n">r_card</span><span class="p">)</span> <span class="o">=</span> <span class="p">(</span>
        <span class="k">self</span><span class="py">.card</span> <span class="k">as</span> <span class="nb">f32</span><span class="p">,</span>
        <span class="n">running</span><span class="py">.card</span> <span class="k">as</span> <span class="nb">f32</span><span class="p">,</span>
        <span class="p">(</span><span class="k">self</span><span class="py">.card</span> <span class="o">-</span> <span class="n">running</span><span class="py">.card</span><span class="p">)</span> <span class="k">as</span> <span class="nb">f32</span><span class="p">,</span>
    <span class="p">);</span>
    <span class="k">let</span> <span class="n">sse_curr</span> <span class="o">=</span> <span class="k">self</span><span class="py">.sum_squares</span> <span class="o">-</span> <span class="k">self</span><span class="py">.sum_linear</span> <span class="o">*</span> <span class="k">self</span><span class="py">.sum_linear</span> <span class="o">/</span> <span class="n">card</span><span class="p">;</span>
    <span class="k">let</span> <span class="n">sse_left</span> <span class="o">=</span> <span class="n">running</span><span class="py">.sum_squares</span> <span class="o">-</span> <span class="n">running</span><span class="py">.sum_linear</span> <span class="o">*</span> <span class="n">running</span><span class="py">.sum_linear</span> <span class="o">/</span> <span class="n">l_card</span><span class="p">;</span>
    <span class="k">let</span> <span class="n">sse_right</span> <span class="o">=</span> <span class="p">(</span><span class="k">self</span><span class="py">.sum_squares</span> <span class="o">-</span> <span class="n">running</span><span class="py">.sum_squares</span><span class="p">)</span>
        <span class="o">-</span> <span class="p">(</span><span class="k">self</span><span class="py">.sum_linear</span> <span class="o">-</span> <span class="n">running</span><span class="py">.sum_linear</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="k">self</span><span class="py">.sum_linear</span> <span class="o">-</span> <span class="n">running</span><span class="py">.sum_linear</span><span class="p">)</span>
            <span class="o">/</span> <span class="n">r_card</span><span class="p">;</span>
    <span class="c1">// weighted variance</span>
    <span class="p">(</span><span class="n">sse_curr</span> <span class="o">-</span> <span class="n">sse_left</span> <span class="o">-</span> <span class="n">sse_right</span><span class="p">)</span> <span class="o">/</span> <span class="n">card</span>
<span class="p">}</span></code></pre></figure>

</details>

<h3 id="defying-gravity">Defying Gravity</h3>

<p>Decision Trees are elegant, interpretable, and surprisingly performant.
Master the base, and the extensions ‚Äî Random Forests, Gradient Boost ‚Äî come almost for free.</p>

<p>The most powerful methods are often the simplest ‚Äî once the base is solid.</p>

<p>Thanks for reading!</p>


    </div>
  </div>

<div class="window">
    <div class="window-header">
      Computational Implementation of Randomized SVD <small>(2025-09-30)</small>
    </div>
    <div class="window-content">
      <p><a href="https://github.com/cyancirrus/stellar-math/blob/main/src/solver/randomized_svd.rs">Randomized SVD Algorithm</a><br />
<a href="https://github.com/cyancirrus/stellar-math/blob/main/src/decomposition/svd.rs">Golub-Kahan Implementation</a><br />
<a href="https://github.com/cyancirrus/stellar-math/blob/main/src/decomposition/qr.rs">Optimized QR Decomposition</a><br />
<a href="https://github.com/cyancirrus/stellar-math/blob/main/src/decomposition/givens.rs">Givens Implementation</a></p>

<p><a href="https://arxiv.org/pdf/0909.4061">Algorithm Source :: N. Halko, P. G. Martinsson, A. Tropp</a></p>

<h2 id="introduction">Introduction</h2>

<p>Singular Value Decomposition (SVD) is one of the cornerstones of numerical, scientific, and statistical computing. It underpins everything from PCA to large-scale linear algebra pipelines, enabling efficient updates and approximations without exploding computational costs.</p>

<p>In this post, we‚Äôll explore SVD from both a <strong>computational</strong> and <strong>statistical</strong> perspective. Specifically, we‚Äôll cover:</p>

<ul>
  <li>Optimizing matrix operations for performance.</li>
  <li>Leveraging statistics for lower-rank approximations.</li>
</ul>

<p>We‚Äôll implement ideas from Halko, Martinsson, and Tropp‚Äôs seminal work <em>Finding Structure in Randomness</em>, and highlight how careful computational tricks make SVD practical at scale.</p>

<h2 id="what-is-svd">What is SVD?</h2>

<p>At a high level, SVD transforms a matrix into three components:</p>

<ol>
  <li><strong>Input rotation (U)</strong></li>
  <li><strong>Scaling (Œ£)</strong></li>
  <li><strong>Output rotation (V·µÄ)</strong></li>
</ol>

<p>Intuitively, SVD identifies the ‚Äúdirections‚Äù in your data that capture the most signal.</p>

<p>For example, suppose we‚Äôre modeling the likelihood of patients revisiting a hospital, given demographic and behavioral variables:</p>

<div class="language-md highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">-</span> age
<span class="p">-</span> gender
<span class="p">-</span> region
<span class="p">-</span> prescription adherence
<span class="p">-</span> preventive care
<span class="p">-</span> previous-year activity
<span class="p">-</span> BMI
<span class="p">-</span> ‚Ä¶plus 20+ other variables
</code></pre></div></div>

<p>The raw input space is large and complex. But often, a few combinations of variables dominate the main signal. Perhaps ‚Äúage √ó preventive care √ó prescription adherence‚Äù explains most of the variance. SVD compresses this multivariate data into latent features: the first singular vector captures the strongest pattern, the next captures the next strongest, and so on.</p>

<p>By truncating to the top <code class="language-plaintext highlighter-rouge">K</code> components, we remove noise while retaining most of the meaningful signal. This is the foundation of PCA, low-rank approximations, and efficient numerical computation in ML pipelines.</p>

<h2 id="implementing-svd">Implementing SVD</h2>

<h3 id="deterministic-svd">Deterministic SVD</h3>

<p>The classic algorithm involves two steps:</p>

<ol>
  <li>
    <p><strong>Bidiagonalization</strong></p>

    <ul>
      <li>Golub-Kahan procedure using Householder reflections.</li>
      <li>Columns below the diagonal and rows beyond the first superdiagonal are zeroed.</li>
    </ul>
  </li>
  <li>
    <p><strong>Bulge chasing</strong></p>

    <ul>
      <li>Givens rotations and extensions diagonalize the bidiagonal matrix.</li>
      <li>This produces the singular values and vectors.</li>
    </ul>
  </li>
</ol>

<blockquote>
  <p>Note: Direct diagonalization without bidiagonalization generally only converges for symmetric positive-definite matrices.</p>
</blockquote>

<h3 id="randomized-svd">Randomized SVD</h3>

<p>Randomized SVD accelerates computations for large matrices by approximating the subspace spanned by the top singular vectors. The procedure is:</p>

<div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>1. Generate a random matrix Œ© ~ (m √ó k)
2. Form Y = (A A') A * Œ©
3. Orthonormalize Y via QR decomposition ‚Üí Q
4. Project A into the smaller subspace: B = Q·µÄ * A
5. Compute deterministic SVD on B
6. Recover approximate U, Œ£, V from the small SVD
</code></pre></div></div>

<p>This reduces computational cost while capturing the dominant signal, which is often all you need in practical applications.</p>

<h2 id="qr-decomposition-the-core-building-block">QR Decomposition: The Core Building Block</h2>

<p>QR decomposition is central to both randomized SVD and Golub-Kahan. Given a matrix <code class="language-plaintext highlighter-rouge">A</code>, we find:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>A = Q * R
</code></pre></div></div>

<p>Where <code class="language-plaintext highlighter-rouge">Q</code> is orthogonal and <code class="language-plaintext highlighter-rouge">R</code> is upper-triangular. Key ideas:</p>

<ul>
  <li>Householder reflections rotate vectors so that elements below the diagonal become zero.</li>
  <li>Reflections are rank-one symmetric matrices: <code class="language-plaintext highlighter-rouge">Q[i] = I - Œ≤ * u * u·µÄ</code>.</li>
  <li>By chaining reflections, we compute <code class="language-plaintext highlighter-rouge">Q = Q[1] * Q[2] * ... * Q[n]</code>.</li>
</ul>

<h4 id="computational-optimization">Computational Optimization</h4>

<p>Naively, applying <code class="language-plaintext highlighter-rouge">Q</code> to another matrix costs <code class="language-plaintext highlighter-rouge">O(n^4)</code> for dense matrices. But by exploiting the structure of Householder reflections, we can reduce this to <code class="language-plaintext highlighter-rouge">O(n^3)</code>:</p>

<div class="language-rust highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">// Householder-based QR update</span>
<span class="k">let</span> <span class="n">w</span> <span class="o">=</span> <span class="n">beta</span> <span class="o">*</span> <span class="n">Q_km1</span> <span class="o">*</span> <span class="n">v_k</span><span class="p">;</span>
<span class="n">A</span> <span class="o">-=</span> <span class="n">w</span> <span class="o">*</span> <span class="n">v_k</span><span class="err">'</span><span class="p">;</span>
</code></pre></div></div>

<p>This optimization is crucial in practice, especially for large-scale pipelines and in Golub-Kahan bidiagonalization.</p>

<h3 id="qr--golub-kahan">QR ‚Üí Golub-Kahan</h3>

<p>Golub-Kahan extends QR ideas to bidiagonalization:</p>

<ul>
  <li>Columns below the diagonal are zeroed (like QR).</li>
  <li>Rows beyond the superdiagonal are zeroed.</li>
  <li>Iterating column and row zeroing produces the bidiagonal matrix.</li>
</ul>

<p>The same optimizations used in QR reduce the cost of these operations, making the algorithm tractable for large matrices.</p>

<h2 id="statistical-optimization-the-randomized-payoff">Statistical Optimization: The Randomized Payoff</h2>

<p>Suppose we have two <code class="language-plaintext highlighter-rouge">10,000 √ó 10,000</code> matrices. Naively, computing <code class="language-plaintext highlighter-rouge">A * B</code> requires <code class="language-plaintext highlighter-rouge">O(n^3) ‚âà 10^12</code> FLOPS‚Äîa trillion operations!</p>

<p>With randomized SVD, if we approximate with rank <code class="language-plaintext highlighter-rouge">k = 1,000</code>:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Flops ‚âà O(n^2 * k) = O(10^8 * 1e3) = 10^11
</code></pre></div></div>

<p>We do only ~10% of the work, while preserving the dominant signal. This dramatically accelerates pipelines in ML and scientific computing, compounding over repeated matrix operations.</p>

<h2 id="wrapping-up">Wrapping Up</h2>

<p>SVD is a powerful tool bridging linear algebra, statistics, and computational optimization. Through careful QR and Golub-Kahan implementations, we can handle large matrices efficiently. Randomized SVD then provides a statistical shortcut, allowing us to approximate dominant structures with far less computation.</p>

<p>The combination of algebraic tricks, computational insights, and statistical reasoning makes this one of the most elegant examples of applied numerical linear algebra.</p>

<p>Explore the <a href="https://github.com/cyancirrus/stellar-math">codebase</a> to see these ideas in action‚Äîand watch the FLOPS disappear.</p>

<p>Thanks for reading!</p>

    </div>
  </div>

<div class="window">
    <div class="window-header">
      K-Nearest Neighbors from Scratch <small>(2025-09-18)</small>
    </div>
    <div class="window-content">
      <p><a href="https://github.com/cyancirrus/stellar-math/blob/main/src/learning/knn.rs">KNN Machine Learning Study</a></p>

<h2 id="introduction">Introduction</h2>

<p>Machine learning and AI are everywhere, so let‚Äôs dive into one of the foundational topics‚Äî<em>not deep learning this time!</em></p>

<p>I wanted to explore one of the most elegant and straightforward algorithms: K-Nearest Neighbors (KNN).</p>

<p>KNN is versatile: it can be used for both regression and classification. It even has connections to electronic engineering through Voronoi diagrams and is ubiquitous in machine learning.</p>

<h2 id="what-is-knn">What is KNN</h2>

<p>KNN has one central premise: <strong>data points close to each other in input space will produce similar outputs.</strong></p>

<ul>
  <li>For regression, we can average the outputs of the neighbors (either simple or weighted).</li>
  <li>For classification, we can either compute probabilities (via softmax) or return the majority vote for a hard classification.</li>
</ul>

<p>This method is extremely clear in its assumptions and behavior. The only requirement is that the data is somewhat continuous and smooth. It‚Äôs a great way to start implementing ML from scratch, without worrying about layers or activation functions, and it also allows for interesting engineering optimizations.</p>

<h2 id="knn-implementation-first-thoughts">KNN Implementation: First Thoughts</h2>

<p>Alright, let‚Äôs implement KNN. But how do we do it?</p>

<p><strong>Knowns:</strong></p>
<ul>
  <li>We need to find the top k closest neighbors for a point.</li>
  <li>We need a distance function.</li>
</ul>

<p>For the distance function, the natural choice is Euclidean distance (triangle distance):</p>

<div class="language-rust highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">fn</span> <span class="nf">distance</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="o">&amp;</span><span class="p">[</span><span class="nb">f64</span><span class="p">],</span> <span class="n">y</span><span class="p">:</span> <span class="o">&amp;</span><span class="p">[</span><span class="nb">f64</span><span class="p">])</span> <span class="k">-&gt;</span> <span class="nb">f64</span> <span class="p">{</span>
    <span class="k">let</span> <span class="k">mut</span> <span class="n">dist</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">;</span>
    <span class="k">for</span> <span class="n">i</span> <span class="k">in</span> <span class="mi">0</span><span class="o">..</span><span class="n">x</span><span class="nf">.len</span><span class="p">()</span> <span class="p">{</span>
        <span class="n">dist</span> <span class="o">+=</span> <span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">])</span><span class="nf">.powi</span><span class="p">(</span><span class="mi">2</span><span class="p">);</span>
    <span class="p">}</span>
    <span class="n">dist</span><span class="nf">.sqrt</span><span class="p">()</span>
<span class="p">}</span>
</code></pre></div></div>

<p>But let‚Äôs think about the cost:</p>

<ul>
  <li>For every point, computing distances to all other points gives us <code class="language-plaintext highlighter-rouge">n * d</code>.</li>
  <li>Sorting distances gives <code class="language-plaintext highlighter-rouge">n log n</code>.</li>
</ul>

<p>So naive complexity is:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>O(n * d + n log n) -&gt; O(n log n)  (dominant term)
</code></pre></div></div>

<p>This is doable for small datasets but not optimal, especially for online or large-scale settings.</p>

<h2 id="the-interesting-part-of-knn">The Interesting Part of KNN</h2>

<p>To find nearest neighbors efficiently, we can partition the space into ‚Äúneighborhoods.‚Äù</p>

<ul>
  <li>One approach: k-d trees.</li>
  <li>Another approach: <strong>Locality Sensitive Hashing (LSH).</strong></li>
</ul>

<h3 id="what-is-lsh">What is LSH?</h3>

<p>LSH is a hashing technique that maps similar inputs to similar outputs. Here‚Äôs the intuition:</p>

<ol>
  <li>Choose a subset of dimensions.</li>
  <li>Add a small random perturbation to avoid boundary misalignment.</li>
  <li>Divide by a bucket width to assign items to discrete buckets.</li>
</ol>

<p>Mathematically:</p>

<div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>a: Standard Normal Vector ~ N(0, I)
b: Standard Uniform Scalar ~ U(0,1)
w: bucket width constant

hash := floor((a¬∑x - b) / w)
</code></pre></div></div>

<p>To improve robustness, we use multiple hash functions:</p>

<div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>H := (hash[0], hash[1], ..., hash[h])
</code></pre></div></div>

<h2 id="performance-analysis">Performance Analysis</h2>

<h3 id="inference">Inference</h3>

<p><strong>Inference steps:</strong></p>

<ol>
  <li>Compute the hash for each function in <code class="language-plaintext highlighter-rouge">H</code> for input <code class="language-plaintext highlighter-rouge">x</code>.</li>
  <li>Retrieve all points from the corresponding buckets.</li>
  <li>Remove duplicates, sort by Euclidean distance, and select top k.</li>
</ol>

<p><strong>Complexity:</strong></p>

<div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>let d = dimensions
let z = number of retrieved neighbors
let h = number of hash functions

O(d*h + h + z*log(z) + z*d) -&gt; O(z log z + C)
</code></pre></div></div>

<p>Compared to naive:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>O(n log n)
</code></pre></div></div>

<p>This approach drastically reduces inference cost for large datasets.</p>

<h3 id="insertion-parsing">Insertion (Parsing)</h3>

<p>For each new vector:</p>

<ol>
  <li>Compute each hash.</li>
  <li>Insert into the hashtable.</li>
</ol>

<p><strong>Complexity:</strong> <code class="language-plaintext highlighter-rouge">O(n * d * h)</code>
<strong>Memory:</strong> <code class="language-plaintext highlighter-rouge">O(n * h)</code></p>

<p>Each point is stored in a bucket for each hash function.</p>

<h2 id="probabilistic-analysis">Probabilistic Analysis</h2>

<p>How do we know LSH will find the correct neighbors?</p>

<p>Let:</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">n</code> = total elements</li>
  <li><code class="language-plaintext highlighter-rouge">b</code> = elements per bucket</li>
  <li><code class="language-plaintext highlighter-rouge">p</code> = probability a hash bucket contains the nearest neighbor</li>
  <li><code class="language-plaintext highlighter-rouge">q</code> = 1 - p</li>
</ul>

<p>With multiple hashes:</p>

<div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pr(1 hash) = p
pr(2 hashes) = 1 - q^2
pr(h hashes) = 1 - q^h
</code></pre></div></div>

<p>As <code class="language-plaintext highlighter-rouge">h ‚Üí ‚àû</code>, probability of finding a neighbor approaches 1.</p>

<p><strong>For LSH with vectors:</strong></p>

<div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>hash(x) = floor((a¬∑x - b)/w)
x* = x + Œµ

|a¬∑(x - x*) / w| = Œµ / w
</code></pre></div></div>

<p>The difference between perturbed vectors scales with <code class="language-plaintext highlighter-rouge">Œµ / w</code>. Probability that two vectors fall into the same bucket:</p>

<div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Pr(|a¬∑(x-y)/w| &lt; 1) = 2 * NormalCDF(w/d) - 1
</code></pre></div></div>

<p>This gives us a statistical bound for neighbor retrieval.</p>

<h2 id="wrapping-up">Wrapping Up</h2>

<p>We‚Äôve explored:</p>

<ul>
  <li>Basic KNN</li>
  <li>Efficient inference with LSH</li>
  <li>Complexity and memory considerations</li>
  <li>Probabilistic guarantees for correctness</li>
</ul>

<p>There are further optimizations in Rust, like using <code class="language-plaintext highlighter-rouge">Arc</code> to avoid cloning data, but those are beyond this post‚Äôs scope.</p>

<p>I hope this post shows how a simple algorithm can become fascinating once you dive into performance and probabilistic analysis. Implementing KNN from scratch with these techniques is both instructive and practical.</p>

<p>Happy coding!</p>

    </div>
  </div>

<div class="window">
    <div class="window-header">
      First Taste of Learnings <small>(2025-06-12)</small>
    </div>
    <div class="window-content">
      <p><a href="https://github.com/cyancirrus/async_feedboard">Rust Fully Asynchronous BlueSky-like study</a></p>

<h2 id="first-async-application-in-rust-and-first-fully-async-backend-ever">First Async Application in Rust (and first fully async backend ever!)</h2>

<p>Recently, I‚Äôve been trying to wrap my head around using async within Rust.<br />
I had some prior experience using async in Python, mainly to make non-blocking calls for embeddings of identified terms when they were independent.<br />
Although I had some background, I was still greatly intimidated - many developers, who appeared far more talented, spoke about how difficult it was to understand Rust‚Äôs async model.</p>

<h3 id="dont-be-scared---jump-in">Don‚Äôt Be Scared - <em>Jump In</em></h3>

<p>Surprisingly, transitioning my message-board-like or BlueSky-like app from a LeetCode solution into a fully async backend wasn‚Äôt terribly difficult.<br />
First, I worked through several async problems focused on using notifications and streaming. Then, without much further practice, I jumped in.</p>

<p>The API structure - being mostly async - was really about learning the tools:</p>
<ul>
  <li><strong>Axum</strong> for the API tree and server</li>
  <li><strong>Serde</strong> to handle JSON and the barrier between client and server</li>
  <li><strong>Tokio</strong> for the async runtime and its async-aware RwLock</li>
  <li><strong>std::sync::Arc</strong> for atomic reference counting so I could clone handles without cloning the actual data</li>
</ul>

<p>But my <em>favorite</em> package - the standout for me‚Ä¶
‚Äì <strong>DashMap</strong> an amazing tool allowing you to manage interior mutability</p>

<h2 id="what-the-data-looked-like-on-the-backend">What the Data Looked Like on the Backend</h2>

<p>One of the main APIs I needed to port was <code class="language-plaintext highlighter-rouge">fn follow(...)</code>, which takes in a <code class="language-plaintext highlighter-rouge">followee_id</code> and a <code class="language-plaintext highlighter-rouge">follower_id</code>.<br />
I wanted users to safely write to their own portion of the data - i.e., a user (the follower) clicks <em>follow</em> on another user (the followee), and we remember this information.</p>

<p>This was tricky to model. The user should only be able to modify their own data. It should also be fully async.</p>

<p>Originally, the data appeared in synchronous code as:</p>
<div class="language-rust highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">HashMap</span><span class="o">&lt;</span><span class="n">UserId</span><span class="p">,</span> <span class="n">HashSet</span><span class="o">&lt;</span><span class="n">UserId</span><span class="o">&gt;&gt;</span>
</code></pre></div></div>
<p>A mapping from user ID to the set of users they follow.<br />
I used a set to enable quick <code class="language-plaintext highlighter-rouge">unfollow</code> (O(1)) and to ensure no duplicate follows.</p>

<h3 id="first-iteration">First Iteration</h3>

<p>To model the problem, I initially reached for a <code class="language-plaintext highlighter-rouge">Mutex</code>.<br />
Mutexes were useful when solving async LeetCode problems, allowing mutation within async code - so I started there.</p>
<div class="language-rust highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">Mutex</span><span class="o">&lt;</span><span class="n">HashSet</span><span class="o">&lt;</span><span class="n">UserId</span><span class="o">&gt;&gt;</span>
</code></pre></div></div>

<p>This worked, but it was blocking.<br />
It was synchronous code masquerading as async.<br />
Time to explore other structures beyond what I‚Äôd seen in my limited Rust async exposure.</p>

<h3 id="enter-rwlock-read-write-lock">Enter RwLock (Read-Write Lock)</h3>

<p>My API could naturally be partitioned into:</p>
<ul>
  <li><strong>Read actions</strong>: <code class="language-plaintext highlighter-rouge">NewsFeed</code></li>
  <li><strong>Write actions</strong>: <code class="language-plaintext highlighter-rouge">Follow</code>, <code class="language-plaintext highlighter-rouge">Publish</code>, <code class="language-plaintext highlighter-rouge">Unfollow</code></li>
</ul>

<p>This seemed like a natural fit for <code class="language-plaintext highlighter-rouge">RwLock</code>, so I implemented:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>RwLock&lt;HashMap&lt;UserId, RwLock&lt;HashSet&lt;UserId&gt;&gt;&gt;&gt;
</code></pre></div></div>

<p>There were only a few code changes: <code class="language-plaintext highlighter-rouge">.lock().await</code> became <code class="language-plaintext highlighter-rouge">.read().await</code> or <code class="language-plaintext highlighter-rouge">.write().await</code>. Overall, the changes were minimal.</p>

<p><code class="language-plaintext highlighter-rouge">RwLock</code> was a major improvement over <code class="language-plaintext highlighter-rouge">Mutex</code> - while <code class="language-plaintext highlighter-rouge">Mutex</code> allows only a single user or thread to access the data at a time, <code class="language-plaintext highlighter-rouge">RwLock</code> allowed multiple readers in parallel.</p>

<p>‚Ä¶but the problem remained: most actions cause side effects (writes), and a single write on the outermost <code class="language-plaintext highlighter-rouge">RwLock</code> blocked the entire backend - even reads!<br />
Multiple users writing to different locations - they <em>should</em> be able to write independently.<br />
This separation had to be modelable. How could I drive that separation?</p>

<h3 id="dashmap-the-sleeper-wizzard">DashMap: The Sleeper Wizzard</h3>

<p>The problem seemed so simple: just enable read-write locking on the interior data.<br />
Enter <strong>DashMap</strong> - like Gandalf cresting the hill at Helm‚Äôs Deep!</p>

<p>DashMap allows users to mutate their private data without needing explicit mutability, and it‚Äôs a near drop-in replacement for <code class="language-plaintext highlighter-rouge">HashMap</code>.</p>

<p>For example:</p>
<div class="language-rust highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">fn</span> <span class="nf">follow</span><span class="p">(</span><span class="o">&amp;</span><span class="k">mut</span> <span class="k">self</span><span class="p">,</span> <span class="n">followee_id</span><span class="p">:</span> <span class="n">UserId</span><span class="p">,</span> <span class="n">follower_id</span><span class="p">:</span> <span class="n">UserId</span><span class="p">)</span>
</code></pre></div></div>
<p>became:</p>
<div class="language-rust highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">fn</span> <span class="nf">follow</span><span class="p">(</span><span class="o">&amp;</span><span class="k">self</span><span class="p">,</span> <span class="n">followee_id</span><span class="p">:</span> <span class="n">UserId</span><span class="p">,</span> <span class="n">follower_id</span><span class="p">:</span> <span class="n">UserId</span><span class="p">)</span>
</code></pre></div></div>

<p>This helped clean up parts of Axum‚Äôs server model and the guarantees needed to build the API tree.<br />
DashMap enabled private mutation - as long as you handled the interior structure correctly, e.g.:</p>
<div class="language-rust highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">DashMap</span><span class="o">&lt;</span><span class="n">UserId</span><span class="p">,</span> <span class="n">RwLock</span><span class="o">&lt;</span><span class="n">HashSet</span><span class="o">&lt;</span><span class="n">UserId</span><span class="o">&gt;&gt;&gt;</span>
</code></pre></div></div>

<p>This was exactly the model I was searching for.<br />
I <em>cannot</em> recommend the library enough if you‚Äôre facing a similar modeling problem where something feels like it <em>should</em> be possible.</p>

<h2 id="takeaway">Takeaway</h2>

<p>Not only is Rust async - and its tooling - becoming ever more mature and viable in production, but‚Ä¶</p>

<p><strong>Don‚Äôt be scared to jump in.</strong><br />
You‚Äôve already solved problems that felt impossible at the time. This is just another challenge.</p>

<p>When I saw my project handle 1,000 posts from 1,000 users and retrieve sorted newsfeeds for 10 users in 12.620865ms seconds on my 2018 machine, I was thrilled.</p>

<p>Programming isn‚Äôt just writing code for things you already know.<br />
Programming <em>is</em> solving new problems, exploring the unknown, and discovering better solutions.</p>

<p>Thanks so much for reading - see you in the next post!</p>

    </div>
  </div>

<div class="window">
    <div class="window-header">
      First Taste of Learnings <small>(2025-06-11)</small>
    </div>
    <div class="window-content">
      <h2 id="welcome-to-my-blog-inaugural-post"><em>Welcome to my blog! Inaugural post!</em></h2>

<p>My name is Autumn, I‚Äôm a mixture of Data Scientist, Software Engineer, and Machine Learning Engineer.
I‚Äôm passionate about mathematics, statistics, performant computing, and low-level code.</p>

<p>I‚Äôve been exploring multiple projects focused on numerical computing and different strategies ‚Äî recently concentrating on matrix multiplication techniques, neural networks, and implementing various algorithms at a low level.</p>

<p>Additionally, I‚Äôve been brushing up on data structures and algorithms to design more performant systems. I have experience with databases, API development, predictive engines, model pipelines, and backend systems for applications.</p>

<p>Here, I hope to track my progress and provide a reference for others making the same journey.
Thanks so much for taking a look at my blog!</p>

    </div>
  </div>

<h2 id="current-studies">Current Studies</h2>
<ul>
  <li><a href="https://github.com/cyancirrus/matix">Pre-optimized scheduler</a></li>
  <li><a href="https://github.com/cyancirrus/stellar-math">BLAS-style math library in Rust</a></li>
  <li><a href="https://github.com/cyancirrus/neural-net">Neural network work-in-progress</a></li>
  <li><a href="https://github.com/cyancirrus/wordle">Fun Wordle dynamic programming</a></li>
</ul>


  </div>
  <script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']]
    },
    svg: {
      fontCache: 'global'
    }
  };
</script>
<script id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>

</body>
</html>

