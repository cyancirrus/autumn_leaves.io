<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Autumn Leaves</title>
  <link rel="stylesheet" href="/style.css" />
</head>
<body>
  <div class="content-wrapper">
    <h1 id="-autumn-leaves">üçÇ Autumn Leaves</h1>

<h2 id="leafnotes-from-a-developer-on-a-journey-through-code-math-and-music">Leafnotes from a developer on a journey through code, math, and music</h2>

<div class="window">
    <div class="window-header">
      Implementation of Tree based Models <small>(2025-10-08)</small>
    </div>
    <div class="window-content">
      <p><a href="https://github.com/cyancirrus/stellar-math/blob/main/src/learning/gradient_boost.rs">Gradient Boost Implementation</a><br />
<a href="https://github.com/cyancirrus/stellar-math/blob/main/src/learning/random_forest.rs">Random Forest Implementation</a><br />
<a href="https://github.com/cyancirrus/stellar-math/blob/main/src/learning/decision_tree.rs">Decision Tree Implementation</a></p>

<h3 id="introduction">Introduction</h3>
<p>Machine learning is based upon a few based structures and methods - the perceptron, the decision tree, expectation maximization, clustering, dimension reduction.
Today we‚Äôll be going through how to implement the following ML Algorithms:</p>

<ul>
  <li>Gradient Boosting</li>
  <li>Random Forests</li>
  <li>Decision Tree</li>
</ul>

<h3 id="a-first-glance-of-the-statistical-performance">A First Glance of the Statistical Performance</h3>

<p><img src="./assets/tree_based_models_total_variance_explained.png" alt="Model Performance" /></p>

<p>The above chart has been drawn from the Boston Housing Data example which for different features predicts the median value of the house. The data is a bit on the smaller types of datasets and has a bit of variance which is important when analyzing the perforamnce of the methods.</p>

<p>Each model was trained 36 times, as there‚Äôs a bit of variance depending upon the subset that is selected (the data is incredibly small).
The measure that i‚Äôve utilized is total variance explained which is defined as</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>// Sum squares error of the model is defined as

SSE(Model) := Sum (yi - yi^)^2
SSE(Data) := Sum (yi - mean)^2

// abbreviated as tve
Total Variance Explained := 1 - SSE(model) / SSE(Data);
</code></pre></div></div>

<p><em>*All TVE metrics are TVE on the unseen data from training.*</em></p>

<p><em>Random Forest</em> - Average TVE 78%</p>
<ul>
  <li>Incredibly strong performance over all iterations this always has a strong perforamnce and is one of the methods which will help to debias the training data in order to help the metho perform well upon new unseen data. Average</li>
</ul>

<p><em>Gradient Boosting</em> - Average TVE 62%</p>
<ul>
  <li>Gradient boosting is normally an incredibly strong technique although the data size has incredibly defeated this method, there‚Äôs not enough data variaty nor enough points in order to recursively fit the data appropriately. The model has incredibly overfit the data, parameters have not been grid searched but several have been tried. This model class is not appropriate for this dataset. Note the massive amount of variance between the lowest score for Gradient Boosted Model compared to it‚Äôs Most Performant, the range is incredibly concerning and appears not stable.</li>
</ul>

<p><em>Decision Tree</em> - Average TVE 67%</p>
<ul>
  <li>Base model and amazing transparency, some overfitting to the sample data which can be seen by it‚Äôs relative performance to RandomForest. Can go through node by node in order to analyze which features were important. Overall a solid pick for the housing data averaging 65% Total Variance Explained for different samplings on the test dataset. Appropriate method for this problem</li>
</ul>

<p><em>In order to run the above models yourself simply clone stellar_math and run</em></p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>// see <span class="k">if </span>you can find better metadata parameters!
cargo run <span class="nt">--example</span> trees
open tve_chart.png
</code></pre></div></div>

<h3 id="preliminaries---what-is-a-decision-tree">Preliminaries - What is a Decision Tree?</h3>

<p>Lets consider the lowly if-else statement, this way we can build intuition for what is a decision tree. Consider the question on whether I should bring an umbrella with my outside‚Ä¶</p>

<div class="language-rust highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">if</span> <span class="n">it_is_raining</span>  <span class="p">{</span> <span class="c1">// then</span>
    <span class="n">items_to_carry</span> <span class="o">+=</span> <span class="s">"umbrella"</span><span class="p">;</span>
<span class="p">}</span> <span class="k">else</span> <span class="p">{</span> <span class="c1">// it is not raining</span>
    <span class="c1">// do not need to carry umbrella</span>
<span class="p">}</span>
</code></pre></div></div>

<p>As we can see the above if-else statement will intelligently help us to decide whether what we should do if it is raining outside.
Lets now extend this if-else statement into the data context, consider the problem of trying to model the quality of a wine.</p>

<p>Consider the factors that would be helpful with predicting quality</p>
<ul>
  <li>country of origin // france, portugal, italy are all known for their wines</li>
  <li>year of production</li>
  <li>average rainfall per country for the year</li>
  <li>average temperature per country for the year</li>
  <li>brand who has produced the wine</li>
  <li>colour - white / red / red-blend</li>
  <li>type of wine which has been produced // merlot, pintot</li>
  <li>price</li>
  <li>Quality &lt;- This will be what we are trying to predict</li>
</ul>

<p>All of these would be very useful and lets imagine the following if condition</p>
<div class="language-rust highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">if</span> <span class="n">country</span> <span class="o">==</span> <span class="n">greece</span> <span class="o">&amp;</span> <span class="n">colour</span> <span class="o">==</span> <span class="n">green</span> <span class="p">{</span>
    <span class="c1">// Greece has amazing white and green wines</span>
    <span class="n">predicted_quality</span> <span class="o">=</span> <span class="mf">1000_f32</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div>

<p>The above simply is the backbone for how decision trees work to predict results. Imagine hundreds of branches which could cover different conditions for all countries and would split the data into different if-then statements intelligently - this is the decision tree.</p>

<p>Algorithmically the decision tree performs the following</p>
<div class="language-rust highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">let</span> <span class="k">mut</span> <span class="n">best_split</span> <span class="o">=</span> <span class="nb">None</span>
<span class="k">let</span> <span class="k">mut</span> <span class="n">explanatory_power</span> <span class="o">=</span>  <span class="mi">0</span><span class="p">;</span>
<span class="k">for</span> <span class="n">_</span> <span class="k">in</span> <span class="mi">0</span><span class="o">..</span><span class="n">desired_number_of_nodes</span> <span class="p">{</span>
    <span class="k">for</span> <span class="n">each</span> <span class="n">dimension</span> <span class="n">find</span> <span class="n">best</span> <span class="n">split</span> <span class="p">{</span>
       <span class="k">if</span> <span class="nf">explanatory_power</span><span class="p">(</span><span class="n">this_split</span><span class="p">)</span> <span class="o">==</span> <span class="n">best</span> <span class="p">{</span>
          <span class="n">best_split</span> <span class="o">=</span> <span class="n">this</span><span class="p">;</span>
          <span class="n">explanatory_power</span> <span class="o">=</span> <span class="nf">explanatory_power</span><span class="p">(</span><span class="n">this_split</span><span class="p">);</span>
       <span class="p">}</span>
    <span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div>

<p>All the above does is formalize the if/else condition, and the predicted <code class="language-plaintext highlighter-rouge">Quality</code> for each node is simply the average quality of the items within the node. Remember to visualize as we are splitting data, the number of items in each node increasingly decreases, so we aren‚Äôt reclassifying the entire data at each step, only the members which are assigned to the current node.</p>

<div class="language-md highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Ie Node[0] // contains all data
<span class="p">-</span> split and now we no longer within training consider Node[0] as it is not a leaf node
Node[1], // contains ~ 1/2 data
Node[2], // contains ~ 1/2 data
<span class="p">-</span> split Node[2]
Node[1], // contains ~ 1/2 data
Node[3], // contains ~ 1/4 data (was previously node 2)
Node[4], // contains ~ 1/4 data (was previously node 2) 
</code></pre></div></div>

<h3 id="decision-tree-extensions">Decision Tree Extensions</h3>

<p>There are two main ways that we can extend the humble decision tree</p>

<h4 id="random-forests">Random Forests</h4>

<p>A random forest is essentially a collection of decision trees. Due to the fact that we deterministically use the best split for a decision tree, we do need to subsample either/or both - the data which are consdiered, or the number of dimensions which are considered at each step.</p>

<p>After training we now have a large number of Decision Trees - apply named a Forest, and in order to perform a prediction</p>
<div class="language-rust highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">let</span> <span class="n">unseen_data</span> <span class="o">=</span> <span class="n">data</span><span class="p">;</span>
<span class="k">let</span> <span class="n">prediction</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
<span class="k">for</span> <span class="n">this_tree</span> <span class="k">in</span> <span class="n">forest</span> <span class="p">{</span>
   <span class="n">prediction</span> <span class="o">+=</span> <span class="n">this_tree</span><span class="nf">.predict</span><span class="p">(</span><span class="n">unseen_data</span><span class="p">);</span>
<span class="p">}</span>
<span class="k">let</span> <span class="n">end_prediction</span> <span class="o">=</span>  <span class="n">prediction</span> <span class="o">/</span> <span class="n">number_of_trees</span>
<span class="n">end_prediction</span>
</code></pre></div></div>

<p>Random forest helps to incredibly debias the overfitting common in decision trees and other tree based methods.</p>

<h4 id="gradient-boosting">Gradient Boosting</h4>

<p>Gradient boosting is the other main way in order to extend decision trees. Instead of creating multiple trees and then averaging the output we recursively fit the error from the previous decision tree.</p>

<p>Think of it like this, we fit the first decision tree, and our prediction for the node isn‚Äôt perfect, so then we look at all of our predictions and then the error. For this remaining error we then fit a new decision tree.</p>

<p>For the end prediction we add sum all of the decision trees output as each decision tree was fitting <code class="language-plaintext highlighter-rouge">y-yhat</code> which when we sum we unfold all of them and get</p>
<div class="language-md highlighter-rouge"><div class="highlight"><pre class="highlight"><code>prediction = y0 + ('y0 - y1) + (y1' - y2) + .. + ('yn-1 - yi);
// each y0-y1 is correcting the error of the previous iteration
prediction = y0 + error_correction[1].. + error_correction[n];
</code></pre></div></div>

<p>The predicted output isn‚Äôt exactly equal, this is how the error correction term is obtained</p>

<h3 id="elphabas-look-at-the-great-wizard-of-oz---tree-extensions-and-their-implementaitons">Elphabas Look at the Great Wizard of Oz - Tree Extensions and their Implementaitons</h3>

<p>Everyone wishes to review the most complex methods implementations - so lets review these first.
<em>Gradient boosting</em> - lets review the complexity! - Wait ‚Ä¶ this looks incredibly simple‚Ä¶</p>
<div class="language-rust highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">use</span> <span class="k">crate</span><span class="p">::</span><span class="nn">learning</span><span class="p">::</span><span class="nn">decision_tree</span><span class="p">::{</span>
    <span class="n">DecisionTree</span><span class="p">,</span>
    <span class="n">DecisionTreeModel</span>
<span class="p">};</span> 
<span class="k">pub</span> <span class="k">struct</span> <span class="n">GradientBoost</span> <span class="p">{</span>
    <span class="n">trees</span><span class="p">:</span><span class="nb">usize</span><span class="p">,</span>
    <span class="n">forest</span><span class="p">:</span> <span class="nb">Vec</span><span class="o">&lt;</span><span class="n">DecisionTreeModel</span><span class="o">&gt;</span><span class="p">,</span>
<span class="p">}</span>
<span class="k">impl</span> <span class="n">GradientBoost</span> <span class="p">{</span>
    <span class="k">pub</span> <span class="k">fn</span> <span class="nf">new</span><span class="p">(</span><span class="n">data</span><span class="p">:</span><span class="o">&amp;</span><span class="k">mut</span> <span class="nb">Vec</span><span class="o">&lt;</span><span class="nb">Vec</span><span class="o">&lt;</span><span class="nb">f32</span><span class="o">&gt;&gt;</span><span class="p">,</span> <span class="n">trees</span><span class="p">:</span><span class="nb">usize</span><span class="p">,</span> <span class="n">nodes</span><span class="p">:</span><span class="nb">usize</span><span class="p">,</span> <span class="n">obs_sample</span><span class="p">:</span><span class="nb">f32</span><span class="p">,</span> <span class="n">dim_sample</span><span class="p">:</span><span class="nb">f32</span><span class="p">)</span> <span class="k">-&gt;</span> <span class="k">Self</span> <span class="p">{</span>
        <span class="k">if</span> <span class="n">data</span><span class="nf">.is_empty</span><span class="p">()</span> <span class="p">||</span> <span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="nf">.is_empty</span><span class="p">()</span> <span class="p">{</span> <span class="nd">panic!</span><span class="p">(</span><span class="s">"data is empty"</span><span class="p">);</span> <span class="p">}</span>
        <span class="k">let</span> <span class="n">n_obs</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="nf">.len</span><span class="p">();</span>
        <span class="k">let</span> <span class="n">dims</span> <span class="o">=</span> <span class="n">data</span><span class="nf">.len</span><span class="p">();</span>
        <span class="k">let</span> <span class="n">target_idx</span> <span class="o">=</span> <span class="n">data</span><span class="nf">.len</span><span class="p">()</span><span class="o">-</span><span class="mi">1</span><span class="p">;</span>
        <span class="k">let</span> <span class="k">mut</span> <span class="n">sample</span> <span class="o">=</span> <span class="nd">vec!</span><span class="p">[</span><span class="mf">0_f32</span><span class="p">;</span> <span class="n">dims</span><span class="p">];</span>
        <span class="k">let</span> <span class="k">mut</span> <span class="n">forest</span> <span class="o">=</span> <span class="nn">Vec</span><span class="p">::</span><span class="nf">with_capacity</span><span class="p">(</span><span class="n">trees</span><span class="p">);</span>
        <span class="k">for</span> <span class="n">_</span> <span class="k">in</span> <span class="mi">0</span><span class="o">..</span><span class="n">trees</span> <span class="p">{</span>
            <span class="k">let</span> <span class="n">tree</span> <span class="o">=</span> <span class="nn">DecisionTree</span><span class="p">::</span><span class="nf">new</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">obs_sample</span><span class="p">,</span> <span class="n">dim_sample</span><span class="p">)</span><span class="nf">.train</span><span class="p">(</span><span class="n">nodes</span><span class="p">);</span>
            <span class="k">for</span> <span class="n">idx</span> <span class="k">in</span> <span class="mi">0</span><span class="o">..</span><span class="n">n_obs</span> <span class="p">{</span>
                <span class="k">for</span> <span class="n">d</span> <span class="k">in</span> <span class="mi">0</span><span class="o">..</span><span class="n">dims</span> <span class="p">{</span> <span class="n">sample</span><span class="p">[</span><span class="n">d</span><span class="p">]</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">d</span><span class="p">][</span><span class="n">idx</span><span class="p">];</span> <span class="p">}</span>
                <span class="k">let</span> <span class="n">pred</span> <span class="o">=</span> <span class="n">tree</span><span class="nf">.predict</span><span class="p">(</span><span class="o">&amp;</span><span class="n">sample</span><span class="p">);</span>
                <span class="n">data</span><span class="p">[</span><span class="n">target_idx</span><span class="p">][</span><span class="n">idx</span><span class="p">]</span> <span class="o">-=</span> <span class="n">pred</span><span class="p">;</span>
            <span class="p">}</span>
            <span class="n">forest</span><span class="nf">.push</span><span class="p">(</span><span class="n">tree</span><span class="p">);</span>
        <span class="p">}</span>
        <span class="k">Self</span><span class="p">{</span>
            <span class="n">trees</span><span class="p">,</span>
            <span class="n">forest</span><span class="p">,</span>
        <span class="p">}</span>
    <span class="p">}</span>
    <span class="k">pub</span> <span class="k">fn</span> <span class="nf">predict</span><span class="p">(</span><span class="o">&amp;</span><span class="k">self</span><span class="p">,</span> <span class="n">data</span><span class="p">:</span><span class="o">&amp;</span><span class="p">[</span><span class="nb">f32</span><span class="p">])</span> <span class="k">-&gt;</span> <span class="nb">f32</span> <span class="p">{</span>
        <span class="k">let</span> <span class="k">mut</span> <span class="n">prediction</span> <span class="o">=</span> <span class="mf">0_f32</span><span class="p">;</span>
        <span class="k">for</span> <span class="n">tree</span> <span class="k">in</span> <span class="o">&amp;</span><span class="k">self</span><span class="py">.forest</span> <span class="p">{</span>
            <span class="n">prediction</span> <span class="o">+=</span> <span class="n">tree</span><span class="nf">.predict</span><span class="p">(</span><span class="n">data</span><span class="p">);</span> 
        <span class="p">}</span>
        <span class="n">prediction</span>
    <span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div>

<p><em>Random forest</em> - this one must be incredibly complex! - Wait‚Ä¶ it‚Äôs exactly as trivial as the Gradient Boost implementation!</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>use crate::learning::decision_tree::{
    DecisionTree,
    DecisionTreeModel
}; 
pub struct RandomForest {
    trees: usize,
    forest: Vec&lt;DecisionTreeModel&gt;,
}
impl RandomForest {
    pub fn new(data:&amp;Vec&lt;Vec&lt;f32&gt;&gt;, trees:usize, nodes:usize, obs_sample:f32, dim_sample:f32) -&gt; Self {
        let forest:Vec&lt;DecisionTreeModel&gt; = (0..trees).into_iter().map(|_| {
            let mut tree = DecisionTree::new(data, obs_sample, dim_sample);
            tree.train(nodes)
        }).collect();
        Self { trees, forest }
    }
    pub fn predict(&amp;self, data:&amp;[f32]) -&gt; f32 {
        let mut cumulative = 0_f32;
        for tree in &amp;self.forest {
            cumulative += tree.predict(data);
        }
        cumulative / self.trees as f32
    }
}
</code></pre></div></div>

<p>Incredibly confusing, that‚Äôs more lackluster than the reveal of Oz! They‚Äôre just wrappers for the decision tree implementation. <em>Remember they *are* model extensions of the decision tree itself</em>.
Should we take a look at the Data Tree implementation?</p>

<h3 id="decision-tree-implementation">Decision Tree Implementation</h3>

<p>Decision Trees actually have a fascinating implementation and their detail connects deeply to the usability of Random Forests and Gradient Boost.</p>

<h3 id="computational-complexity">Computational Complexity</h3>

<p>For each decision tree we must look at every potential split. Lets consider the idea - for each feature create a list of the input rows, but sorted by the dimension values themself.</p>

<p>Total cost for creating the sorted list for every dimension <code class="language-plaintext highlighter-rouge">d</code>.</p>
<div class="language-md highlighter-rouge"><div class="highlight"><pre class="highlight"><code>// Sort's cost is O( n log n);
O( d <span class="err">*</span> n log n )
</code></pre></div></div>
<p>For this implementation I will be looking at the Sum of Squared Errors (SSE). For each dimension in order to find the best split we partition a current node‚Äôs values into two nodes. As the statement is if-else, the split node will partition the data into two regions</p>

<div class="language-md highlighter-rouge"><div class="highlight"><pre class="highlight"><code>//For each member of the node

left_node := dimension_value &lt;= partition_value;
right_node := dimension_value &gt; partition_value;
</code></pre></div></div>

<p>We can use the following relation for variance</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>BaseNode := unsplit_node
BaseNode ::
  sum_squares // sum of y[i] * y[i] for members of this node
  sum_linear // sum of y[i] for members of the node

BaseNode SSE := sum_squares - sum_linear * sum_linear / cardinality; // cardinality is just number of members;


// This will be the new variance if we were to split at this given point
SplitNodeSSE = SSE(left_node) + SSE(right_node)

// We calculate this on a running basis
LeftNode SSE := sum_squares - sum_linear * sum_linear  / cardinality;

// And now we can derive the right nodes new SSE
inferred_sum_squares = BaseNode.sum_squares - LeftNode.sum_squares;
inferred_sum_linear = BaseNode.sum_linear - LeftNode.sum_linear;
inferred_cardinality = BaseNode.cardinality - LeftNode.cardinality;

RightNode SSE := inferred_sum_squares - inferred_sum_linear * inferred_sum_linear / inferred_cardinality; 

</code></pre></div></div>

<p>Amazing! this now means that for each potential split that our total cost for determining the optimal split is actually proporitional to the number of records!
This is <em>la coup de grace</em> of the implementation. This is what enable the implementation to be performant and directly drives the reason for the sorted dimensions.</p>

<p>This means that we can simply iterate over the sorted dimensions and check if our split is optimal, linearly!
The only caveat is that when we find the new split, we simply need to sort the other dimensions. I find this incredibly beautiful and elegant, lets look at the cost.</p>

<div class="language-md highlighter-rouge"><div class="highlight"><pre class="highlight"><code>// cost for determining the optimal split for
let n := number of observations in the data;
let s := number of desired nodes // splits
let d := number of dimensions considered

// cost for all splitting of nodes
// O( find_the_best_split + sorting_other_dimensions)

O( s <span class="ge">* n *</span> d  + d <span class="err">*</span> logn)
</code></pre></div></div>

<p>Finally we can show that the total cost for training the entire decision tree occurs</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>// O( initial_sort_dimension_cost + splitting_of_nodes )
O( d * nlog(n) + s * n * d)

// s should be &lt;&lt; log(n)

// End Cost -- Assomptitically equal to the initial cost of sorting the dimensions!!
O( d * nlog(n));
</code></pre></div></div>

<h3 id="defying-gravity">Defying Gravity</h3>

<p>Decision Trees are amazingly powerful tools, the most important part is to ensure that a coherent and optimal decision tree base, then the implementations are more straightforward than Shiz Universities‚Äô reaction to Elphaba‚Äôs green skin!</p>

<p>Thanks for reading!</p>

    </div>
  </div>

<div class="window">
    <div class="window-header">
      Computational Implementation of Randomized SVD <small>(2025-09-30)</small>
    </div>
    <div class="window-content">
      <p><a href="https://github.com/cyancirrus/stellar-math/blob/main/src/solver/randomized_svd.rs">Randomized SVD Algorithm</a><br />
<a href="https://github.com/cyancirrus/stellar-math/blob/main/src/decomposition/svd.rs">Golub-Kahan Implementation</a><br />
<a href="https://github.com/cyancirrus/stellar-math/blob/main/src/decomposition/qr.rs">Optimized QR Decomposition</a><br />
<a href="https://github.com/cyancirrus/stellar-math/blob/main/src/decomposition/givens.rs">Givens Implementation</a></p>

<p><a href="https://arxiv.org/pdf/0909.4061">Algorithm Source :: N. Halko, P. G. Martinsson, A. Tropp</a></p>

<h2 id="introduction">Introduction</h2>

<p>Singular Value Decomposition (SVD) is one of the cornerstones of numerical, scientific, and statistical computing. It underpins everything from PCA to large-scale linear algebra pipelines, enabling efficient updates and approximations without exploding computational costs.</p>

<p>In this post, we‚Äôll explore SVD from both a <strong>computational</strong> and <strong>statistical</strong> perspective. Specifically, we‚Äôll cover:</p>

<ul>
  <li>Optimizing matrix operations for performance.</li>
  <li>Leveraging statistics for lower-rank approximations.</li>
</ul>

<p>We‚Äôll implement ideas from Halko, Martinsson, and Tropp‚Äôs seminal work <em>Finding Structure in Randomness</em>, and highlight how careful computational tricks make SVD practical at scale.</p>

<h2 id="what-is-svd">What is SVD?</h2>

<p>At a high level, SVD transforms a matrix into three components:</p>

<ol>
  <li><strong>Input rotation (U)</strong></li>
  <li><strong>Scaling (Œ£)</strong></li>
  <li><strong>Output rotation (V·µÄ)</strong></li>
</ol>

<p>Intuitively, SVD identifies the ‚Äúdirections‚Äù in your data that capture the most signal.</p>

<p>For example, suppose we‚Äôre modeling the likelihood of patients revisiting a hospital, given demographic and behavioral variables:</p>

<div class="language-md highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">-</span> age
<span class="p">-</span> gender
<span class="p">-</span> region
<span class="p">-</span> prescription adherence
<span class="p">-</span> preventive care
<span class="p">-</span> previous-year activity
<span class="p">-</span> BMI
<span class="p">-</span> ‚Ä¶plus 20+ other variables
</code></pre></div></div>

<p>The raw input space is large and complex. But often, a few combinations of variables dominate the main signal. Perhaps ‚Äúage √ó preventive care √ó prescription adherence‚Äù explains most of the variance. SVD compresses this multivariate data into latent features: the first singular vector captures the strongest pattern, the next captures the next strongest, and so on.</p>

<p>By truncating to the top <code class="language-plaintext highlighter-rouge">K</code> components, we remove noise while retaining most of the meaningful signal. This is the foundation of PCA, low-rank approximations, and efficient numerical computation in ML pipelines.</p>

<h2 id="implementing-svd">Implementing SVD</h2>

<h3 id="deterministic-svd">Deterministic SVD</h3>

<p>The classic algorithm involves two steps:</p>

<ol>
  <li>
    <p><strong>Bidiagonalization</strong></p>

    <ul>
      <li>Golub-Kahan procedure using Householder reflections.</li>
      <li>Columns below the diagonal and rows beyond the first superdiagonal are zeroed.</li>
    </ul>
  </li>
  <li>
    <p><strong>Bulge chasing</strong></p>

    <ul>
      <li>Givens rotations and extensions diagonalize the bidiagonal matrix.</li>
      <li>This produces the singular values and vectors.</li>
    </ul>
  </li>
</ol>

<blockquote>
  <p>Note: Direct diagonalization without bidiagonalization generally only converges for symmetric positive-definite matrices.</p>
</blockquote>

<h3 id="randomized-svd">Randomized SVD</h3>

<p>Randomized SVD accelerates computations for large matrices by approximating the subspace spanned by the top singular vectors. The procedure is:</p>

<div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>1. Generate a random matrix Œ© ~ (m √ó k)
2. Form Y = (A A') A * Œ©
3. Orthonormalize Y via QR decomposition ‚Üí Q
4. Project A into the smaller subspace: B = Q·µÄ * A
5. Compute deterministic SVD on B
6. Recover approximate U, Œ£, V from the small SVD
</code></pre></div></div>

<p>This reduces computational cost while capturing the dominant signal, which is often all you need in practical applications.</p>

<h2 id="qr-decomposition-the-core-building-block">QR Decomposition: The Core Building Block</h2>

<p>QR decomposition is central to both randomized SVD and Golub-Kahan. Given a matrix <code class="language-plaintext highlighter-rouge">A</code>, we find:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>A = Q * R
</code></pre></div></div>

<p>Where <code class="language-plaintext highlighter-rouge">Q</code> is orthogonal and <code class="language-plaintext highlighter-rouge">R</code> is upper-triangular. Key ideas:</p>

<ul>
  <li>Householder reflections rotate vectors so that elements below the diagonal become zero.</li>
  <li>Reflections are rank-one symmetric matrices: <code class="language-plaintext highlighter-rouge">Q[i] = I - Œ≤ * u * u·µÄ</code>.</li>
  <li>By chaining reflections, we compute <code class="language-plaintext highlighter-rouge">Q = Q[1] * Q[2] * ... * Q[n]</code>.</li>
</ul>

<h4 id="computational-optimization">Computational Optimization</h4>

<p>Naively, applying <code class="language-plaintext highlighter-rouge">Q</code> to another matrix costs <code class="language-plaintext highlighter-rouge">O(n^4)</code> for dense matrices. But by exploiting the structure of Householder reflections, we can reduce this to <code class="language-plaintext highlighter-rouge">O(n^3)</code>:</p>

<div class="language-rust highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">// Householder-based QR update</span>
<span class="k">let</span> <span class="n">w</span> <span class="o">=</span> <span class="n">beta</span> <span class="o">*</span> <span class="n">Q_km1</span> <span class="o">*</span> <span class="n">v_k</span><span class="p">;</span>
<span class="n">A</span> <span class="o">-=</span> <span class="n">w</span> <span class="o">*</span> <span class="n">v_k</span><span class="err">'</span><span class="p">;</span>
</code></pre></div></div>

<p>This optimization is crucial in practice, especially for large-scale pipelines and in Golub-Kahan bidiagonalization.</p>

<h3 id="qr--golub-kahan">QR ‚Üí Golub-Kahan</h3>

<p>Golub-Kahan extends QR ideas to bidiagonalization:</p>

<ul>
  <li>Columns below the diagonal are zeroed (like QR).</li>
  <li>Rows beyond the superdiagonal are zeroed.</li>
  <li>Iterating column and row zeroing produces the bidiagonal matrix.</li>
</ul>

<p>The same optimizations used in QR reduce the cost of these operations, making the algorithm tractable for large matrices.</p>

<h2 id="statistical-optimization-the-randomized-payoff">Statistical Optimization: The Randomized Payoff</h2>

<p>Suppose we have two <code class="language-plaintext highlighter-rouge">10,000 √ó 10,000</code> matrices. Naively, computing <code class="language-plaintext highlighter-rouge">A * B</code> requires <code class="language-plaintext highlighter-rouge">O(n^3) ‚âà 10^12</code> FLOPS‚Äîa trillion operations!</p>

<p>With randomized SVD, if we approximate with rank <code class="language-plaintext highlighter-rouge">k = 1,000</code>:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Flops ‚âà O(n^2 * k) = O(10^8 * 1e3) = 10^11
</code></pre></div></div>

<p>We do only ~10% of the work, while preserving the dominant signal. This dramatically accelerates pipelines in ML and scientific computing, compounding over repeated matrix operations.</p>

<h2 id="wrapping-up">Wrapping Up</h2>

<p>SVD is a powerful tool bridging linear algebra, statistics, and computational optimization. Through careful QR and Golub-Kahan implementations, we can handle large matrices efficiently. Randomized SVD then provides a statistical shortcut, allowing us to approximate dominant structures with far less computation.</p>

<p>The combination of algebraic tricks, computational insights, and statistical reasoning makes this one of the most elegant examples of applied numerical linear algebra.</p>

<p>Explore the <a href="https://github.com/cyancirrus/stellar-math">codebase</a> to see these ideas in action‚Äîand watch the FLOPS disappear.</p>

<p>Thanks for reading!</p>

    </div>
  </div>

<div class="window">
    <div class="window-header">
      K-Nearest Neighbors from Scratch <small>(2025-09-18)</small>
    </div>
    <div class="window-content">
      <p><a href="https://github.com/cyancirrus/stellar-math/blob/main/src/learning/knn.rs">KNN Machine Learning Study</a></p>

<h2 id="introduction">Introduction</h2>

<p>Machine learning and AI are everywhere, so let‚Äôs dive into one of the foundational topics‚Äî<em>not deep learning this time!</em></p>

<p>I wanted to explore one of the most elegant and straightforward algorithms: K-Nearest Neighbors (KNN).</p>

<p>KNN is versatile: it can be used for both regression and classification. It even has connections to electronic engineering through Voronoi diagrams and is ubiquitous in machine learning.</p>

<h2 id="what-is-knn">What is KNN</h2>

<p>KNN has one central premise: <strong>data points close to each other in input space will produce similar outputs.</strong></p>

<ul>
  <li>For regression, we can average the outputs of the neighbors (either simple or weighted).</li>
  <li>For classification, we can either compute probabilities (via softmax) or return the majority vote for a hard classification.</li>
</ul>

<p>This method is extremely clear in its assumptions and behavior. The only requirement is that the data is somewhat continuous and smooth. It‚Äôs a great way to start implementing ML from scratch, without worrying about layers or activation functions, and it also allows for interesting engineering optimizations.</p>

<h2 id="knn-implementation-first-thoughts">KNN Implementation: First Thoughts</h2>

<p>Alright, let‚Äôs implement KNN. But how do we do it?</p>

<p><strong>Knowns:</strong></p>
<ul>
  <li>We need to find the top k closest neighbors for a point.</li>
  <li>We need a distance function.</li>
</ul>

<p>For the distance function, the natural choice is Euclidean distance (triangle distance):</p>

<div class="language-rust highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">fn</span> <span class="nf">distance</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="o">&amp;</span><span class="p">[</span><span class="nb">f64</span><span class="p">],</span> <span class="n">y</span><span class="p">:</span> <span class="o">&amp;</span><span class="p">[</span><span class="nb">f64</span><span class="p">])</span> <span class="k">-&gt;</span> <span class="nb">f64</span> <span class="p">{</span>
    <span class="k">let</span> <span class="k">mut</span> <span class="n">dist</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">;</span>
    <span class="k">for</span> <span class="n">i</span> <span class="k">in</span> <span class="mi">0</span><span class="o">..</span><span class="n">x</span><span class="nf">.len</span><span class="p">()</span> <span class="p">{</span>
        <span class="n">dist</span> <span class="o">+=</span> <span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">])</span><span class="nf">.powi</span><span class="p">(</span><span class="mi">2</span><span class="p">);</span>
    <span class="p">}</span>
    <span class="n">dist</span><span class="nf">.sqrt</span><span class="p">()</span>
<span class="p">}</span>
</code></pre></div></div>

<p>But let‚Äôs think about the cost:</p>

<ul>
  <li>For every point, computing distances to all other points gives us <code class="language-plaintext highlighter-rouge">n * d</code>.</li>
  <li>Sorting distances gives <code class="language-plaintext highlighter-rouge">n log n</code>.</li>
</ul>

<p>So naive complexity is:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>O(n * d + n log n) -&gt; O(n log n)  (dominant term)
</code></pre></div></div>

<p>This is doable for small datasets but not optimal, especially for online or large-scale settings.</p>

<h2 id="the-interesting-part-of-knn">The Interesting Part of KNN</h2>

<p>To find nearest neighbors efficiently, we can partition the space into ‚Äúneighborhoods.‚Äù</p>

<ul>
  <li>One approach: k-d trees.</li>
  <li>Another approach: <strong>Locality Sensitive Hashing (LSH).</strong></li>
</ul>

<h3 id="what-is-lsh">What is LSH?</h3>

<p>LSH is a hashing technique that maps similar inputs to similar outputs. Here‚Äôs the intuition:</p>

<ol>
  <li>Choose a subset of dimensions.</li>
  <li>Add a small random perturbation to avoid boundary misalignment.</li>
  <li>Divide by a bucket width to assign items to discrete buckets.</li>
</ol>

<p>Mathematically:</p>

<div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>a: Standard Normal Vector ~ N(0, I)
b: Standard Uniform Scalar ~ U(0,1)
w: bucket width constant

hash := floor((a¬∑x - b) / w)
</code></pre></div></div>

<p>To improve robustness, we use multiple hash functions:</p>

<div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>H := (hash[0], hash[1], ..., hash[h])
</code></pre></div></div>

<h2 id="performance-analysis">Performance Analysis</h2>

<h3 id="inference">Inference</h3>

<p><strong>Inference steps:</strong></p>

<ol>
  <li>Compute the hash for each function in <code class="language-plaintext highlighter-rouge">H</code> for input <code class="language-plaintext highlighter-rouge">x</code>.</li>
  <li>Retrieve all points from the corresponding buckets.</li>
  <li>Remove duplicates, sort by Euclidean distance, and select top k.</li>
</ol>

<p><strong>Complexity:</strong></p>

<div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>let d = dimensions
let z = number of retrieved neighbors
let h = number of hash functions

O(d*h + h + z*log(z) + z*d) -&gt; O(z log z + C)
</code></pre></div></div>

<p>Compared to naive:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>O(n log n)
</code></pre></div></div>

<p>This approach drastically reduces inference cost for large datasets.</p>

<h3 id="insertion-parsing">Insertion (Parsing)</h3>

<p>For each new vector:</p>

<ol>
  <li>Compute each hash.</li>
  <li>Insert into the hashtable.</li>
</ol>

<p><strong>Complexity:</strong> <code class="language-plaintext highlighter-rouge">O(n * d * h)</code>
<strong>Memory:</strong> <code class="language-plaintext highlighter-rouge">O(n * h)</code></p>

<p>Each point is stored in a bucket for each hash function.</p>

<h2 id="probabilistic-analysis">Probabilistic Analysis</h2>

<p>How do we know LSH will find the correct neighbors?</p>

<p>Let:</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">n</code> = total elements</li>
  <li><code class="language-plaintext highlighter-rouge">b</code> = elements per bucket</li>
  <li><code class="language-plaintext highlighter-rouge">p</code> = probability a hash bucket contains the nearest neighbor</li>
  <li><code class="language-plaintext highlighter-rouge">q</code> = 1 - p</li>
</ul>

<p>With multiple hashes:</p>

<div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pr(1 hash) = p
pr(2 hashes) = 1 - q^2
pr(h hashes) = 1 - q^h
</code></pre></div></div>

<p>As <code class="language-plaintext highlighter-rouge">h ‚Üí ‚àû</code>, probability of finding a neighbor approaches 1.</p>

<p><strong>For LSH with vectors:</strong></p>

<div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>hash(x) = floor((a¬∑x - b)/w)
x* = x + Œµ

|a¬∑(x - x*) / w| = Œµ / w
</code></pre></div></div>

<p>The difference between perturbed vectors scales with <code class="language-plaintext highlighter-rouge">Œµ / w</code>. Probability that two vectors fall into the same bucket:</p>

<div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Pr(|a¬∑(x-y)/w| &lt; 1) = 2 * NormalCDF(w/d) - 1
</code></pre></div></div>

<p>This gives us a statistical bound for neighbor retrieval.</p>

<h2 id="wrapping-up">Wrapping Up</h2>

<p>We‚Äôve explored:</p>

<ul>
  <li>Basic KNN</li>
  <li>Efficient inference with LSH</li>
  <li>Complexity and memory considerations</li>
  <li>Probabilistic guarantees for correctness</li>
</ul>

<p>There are further optimizations in Rust, like using <code class="language-plaintext highlighter-rouge">Arc</code> to avoid cloning data, but those are beyond this post‚Äôs scope.</p>

<p>I hope this post shows how a simple algorithm can become fascinating once you dive into performance and probabilistic analysis. Implementing KNN from scratch with these techniques is both instructive and practical.</p>

<p>Happy coding!</p>

    </div>
  </div>

<div class="window">
    <div class="window-header">
      First Taste of Learnings <small>(2025-06-12)</small>
    </div>
    <div class="window-content">
      <p><a href="https://github.com/cyancirrus/async_feedboard">Rust Fully Asynchronous BlueSky-like study</a></p>

<h2 id="first-async-application-in-rust-and-first-fully-async-backend-ever">First Async Application in Rust (and first fully async backend ever!)</h2>

<p>Recently, I‚Äôve been trying to wrap my head around using async within Rust.<br />
I had some prior experience using async in Python, mainly to make non-blocking calls for embeddings of identified terms when they were independent.<br />
Although I had some background, I was still greatly intimidated - many developers, who appeared far more talented, spoke about how difficult it was to understand Rust‚Äôs async model.</p>

<h3 id="dont-be-scared---jump-in">Don‚Äôt Be Scared - <em>Jump In</em></h3>

<p>Surprisingly, transitioning my message-board-like or BlueSky-like app from a LeetCode solution into a fully async backend wasn‚Äôt terribly difficult.<br />
First, I worked through several async problems focused on using notifications and streaming. Then, without much further practice, I jumped in.</p>

<p>The API structure - being mostly async - was really about learning the tools:</p>
<ul>
  <li><strong>Axum</strong> for the API tree and server</li>
  <li><strong>Serde</strong> to handle JSON and the barrier between client and server</li>
  <li><strong>Tokio</strong> for the async runtime and its async-aware RwLock</li>
  <li><strong>std::sync::Arc</strong> for atomic reference counting so I could clone handles without cloning the actual data</li>
</ul>

<p>But my <em>favorite</em> package - the standout for me‚Ä¶
‚Äì <strong>DashMap</strong> an amazing tool allowing you to manage interior mutability</p>

<h2 id="what-the-data-looked-like-on-the-backend">What the Data Looked Like on the Backend</h2>

<p>One of the main APIs I needed to port was <code class="language-plaintext highlighter-rouge">fn follow(...)</code>, which takes in a <code class="language-plaintext highlighter-rouge">followee_id</code> and a <code class="language-plaintext highlighter-rouge">follower_id</code>.<br />
I wanted users to safely write to their own portion of the data - i.e., a user (the follower) clicks <em>follow</em> on another user (the followee), and we remember this information.</p>

<p>This was tricky to model. The user should only be able to modify their own data. It should also be fully async.</p>

<p>Originally, the data appeared in synchronous code as:</p>
<div class="language-rust highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">HashMap</span><span class="o">&lt;</span><span class="n">UserId</span><span class="p">,</span> <span class="n">HashSet</span><span class="o">&lt;</span><span class="n">UserId</span><span class="o">&gt;&gt;</span>
</code></pre></div></div>
<p>A mapping from user ID to the set of users they follow.<br />
I used a set to enable quick <code class="language-plaintext highlighter-rouge">unfollow</code> (O(1)) and to ensure no duplicate follows.</p>

<h3 id="first-iteration">First Iteration</h3>

<p>To model the problem, I initially reached for a <code class="language-plaintext highlighter-rouge">Mutex</code>.<br />
Mutexes were useful when solving async LeetCode problems, allowing mutation within async code - so I started there.</p>
<div class="language-rust highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">Mutex</span><span class="o">&lt;</span><span class="n">HashSet</span><span class="o">&lt;</span><span class="n">UserId</span><span class="o">&gt;&gt;</span>
</code></pre></div></div>

<p>This worked, but it was blocking.<br />
It was synchronous code masquerading as async.<br />
Time to explore other structures beyond what I‚Äôd seen in my limited Rust async exposure.</p>

<h3 id="enter-rwlock-read-write-lock">Enter RwLock (Read-Write Lock)</h3>

<p>My API could naturally be partitioned into:</p>
<ul>
  <li><strong>Read actions</strong>: <code class="language-plaintext highlighter-rouge">NewsFeed</code></li>
  <li><strong>Write actions</strong>: <code class="language-plaintext highlighter-rouge">Follow</code>, <code class="language-plaintext highlighter-rouge">Publish</code>, <code class="language-plaintext highlighter-rouge">Unfollow</code></li>
</ul>

<p>This seemed like a natural fit for <code class="language-plaintext highlighter-rouge">RwLock</code>, so I implemented:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>RwLock&lt;HashMap&lt;UserId, RwLock&lt;HashSet&lt;UserId&gt;&gt;&gt;&gt;
</code></pre></div></div>

<p>There were only a few code changes: <code class="language-plaintext highlighter-rouge">.lock().await</code> became <code class="language-plaintext highlighter-rouge">.read().await</code> or <code class="language-plaintext highlighter-rouge">.write().await</code>. Overall, the changes were minimal.</p>

<p><code class="language-plaintext highlighter-rouge">RwLock</code> was a major improvement over <code class="language-plaintext highlighter-rouge">Mutex</code> - while <code class="language-plaintext highlighter-rouge">Mutex</code> allows only a single user or thread to access the data at a time, <code class="language-plaintext highlighter-rouge">RwLock</code> allowed multiple readers in parallel.</p>

<p>‚Ä¶but the problem remained: most actions cause side effects (writes), and a single write on the outermost <code class="language-plaintext highlighter-rouge">RwLock</code> blocked the entire backend - even reads!<br />
Multiple users writing to different locations - they <em>should</em> be able to write independently.<br />
This separation had to be modelable. How could I drive that separation?</p>

<h3 id="dashmap-the-sleeper-wizzard">DashMap: The Sleeper Wizzard</h3>

<p>The problem seemed so simple: just enable read-write locking on the interior data.<br />
Enter <strong>DashMap</strong> - like Gandalf cresting the hill at Helm‚Äôs Deep!</p>

<p>DashMap allows users to mutate their private data without needing explicit mutability, and it‚Äôs a near drop-in replacement for <code class="language-plaintext highlighter-rouge">HashMap</code>.</p>

<p>For example:</p>
<div class="language-rust highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">fn</span> <span class="nf">follow</span><span class="p">(</span><span class="o">&amp;</span><span class="k">mut</span> <span class="k">self</span><span class="p">,</span> <span class="n">followee_id</span><span class="p">:</span> <span class="n">UserId</span><span class="p">,</span> <span class="n">follower_id</span><span class="p">:</span> <span class="n">UserId</span><span class="p">)</span>
</code></pre></div></div>
<p>became:</p>
<div class="language-rust highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">fn</span> <span class="nf">follow</span><span class="p">(</span><span class="o">&amp;</span><span class="k">self</span><span class="p">,</span> <span class="n">followee_id</span><span class="p">:</span> <span class="n">UserId</span><span class="p">,</span> <span class="n">follower_id</span><span class="p">:</span> <span class="n">UserId</span><span class="p">)</span>
</code></pre></div></div>

<p>This helped clean up parts of Axum‚Äôs server model and the guarantees needed to build the API tree.<br />
DashMap enabled private mutation - as long as you handled the interior structure correctly, e.g.:</p>
<div class="language-rust highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">DashMap</span><span class="o">&lt;</span><span class="n">UserId</span><span class="p">,</span> <span class="n">RwLock</span><span class="o">&lt;</span><span class="n">HashSet</span><span class="o">&lt;</span><span class="n">UserId</span><span class="o">&gt;&gt;&gt;</span>
</code></pre></div></div>

<p>This was exactly the model I was searching for.<br />
I <em>cannot</em> recommend the library enough if you‚Äôre facing a similar modeling problem where something feels like it <em>should</em> be possible.</p>

<h2 id="takeaway">Takeaway</h2>

<p>Not only is Rust async - and its tooling - becoming ever more mature and viable in production, but‚Ä¶</p>

<p><strong>Don‚Äôt be scared to jump in.</strong><br />
You‚Äôve already solved problems that felt impossible at the time. This is just another challenge.</p>

<p>When I saw my project handle 1,000 posts from 1,000 users and retrieve sorted newsfeeds for 10 users in 12.620865ms seconds on my 2018 machine, I was thrilled.</p>

<p>Programming isn‚Äôt just writing code for things you already know.<br />
Programming <em>is</em> solving new problems, exploring the unknown, and discovering better solutions.</p>

<p>Thanks so much for reading - see you in the next post!</p>

    </div>
  </div>

<div class="window">
    <div class="window-header">
      First Taste of Learnings <small>(2025-06-11)</small>
    </div>
    <div class="window-content">
      <h2 id="welcome-to-my-blog-inaugural-post"><em>Welcome to my blog! Inaugural post!</em></h2>

<p>My name is Autumn, I‚Äôm a mixture of Data Scientist, Software Engineer, and Machine Learning Engineer.
I‚Äôm passionate about mathematics, statistics, performant computing, and low-level code.</p>

<p>I‚Äôve been exploring multiple projects focused on numerical computing and different strategies ‚Äî recently concentrating on matrix multiplication techniques, neural networks, and implementing various algorithms at a low level.</p>

<p>Additionally, I‚Äôve been brushing up on data structures and algorithms to design more performant systems. I have experience with databases, API development, predictive engines, model pipelines, and backend systems for applications.</p>

<p>Here, I hope to track my progress and provide a reference for others making the same journey.
Thanks so much for taking a look at my blog!</p>

    </div>
  </div>

<h2 id="current-studies">Current Studies</h2>
<ul>
  <li><a href="https://github.com/cyancirrus/matix">Pre-optimized scheduler</a></li>
  <li><a href="https://github.com/cyancirrus/stellar-math">Blas style math lib in rust</a></li>
  <li><a href="https://github.com/cyancirrus/neural-net">Neural net work-in-progress</a></li>
  <li><a href="https://github.com/cyancirrus/wordle">Fun wordle dynamic programming</a></li>
</ul>

  </div>
</body>
</html>

