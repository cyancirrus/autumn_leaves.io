<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Autumn Leaves</title>
  <link rel="stylesheet" href="/style.css" />
</head>
<body>
  <div class="content-wrapper">
    <h1 id="-autumn-leaves">üçÇ Autumn Leaves</h1>

<h2 id="leafnotes-from-a-developer-on-a-journey-through-code-math-and-music">Leafnotes from a developer on a journey through code, math, and music</h2>

<div class="window">
    <div class="window-header">
      K-Nearest Neighbors from Scratch <small>(2025-06-12)</small>
    </div>
    <div class="window-content">
      <p><a href="https://github.com/cyancirrus/stellar-math/blob/main/src/learning/knn.rs">KNN Machine Learning Study</a></p>

<h2 id="introduction">Introduction</h2>

<p>Machine learning and AI are everywhere, so let‚Äôs dive into one of the foundational topics‚Äî<em>not deep learning this time!</em> :)</p>

<p>I wanted to explore one of the most elegant and straightforward algorithms: K-Nearest Neighbors (KNN).</p>

<p>KNN is versatile: it can be used for both regression and classification. It even has connections to electronic engineering through Voronoi diagrams and is ubiquitous in machine learning.</p>

<h2 id="what-is-knn">What is KNN</h2>

<p>KNN has one central premise: <strong>data points close to each other in input space will produce similar outputs.</strong></p>

<ul>
  <li>For regression, we can average the outputs of the neighbors (either simple or weighted).</li>
  <li>For classification, we can either compute probabilities (via softmax) or return the majority vote for a hard classification.</li>
</ul>

<p>This method is extremely clear in its assumptions and behavior. The only requirement is that the data is somewhat continuous and smooth. It‚Äôs a great way to start implementing ML from scratch, without worrying about layers or activation functions, and it also allows for interesting engineering optimizations.</p>

<h2 id="knn-implementation-first-thoughts">KNN Implementation: First Thoughts</h2>

<p>Alright, let‚Äôs implement KNN. But how do we do it?</p>

<p><strong>Knowns:</strong></p>
<ul>
  <li>We need to find the top k closest neighbors for a point.</li>
  <li>We need a distance function.</li>
</ul>

<p>For the distance function, the natural choice is Euclidean distance (triangle distance):</p>

<div class="language-rust highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">fn</span> <span class="nf">distance</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="o">&amp;</span><span class="p">[</span><span class="nb">f64</span><span class="p">],</span> <span class="n">y</span><span class="p">:</span> <span class="o">&amp;</span><span class="p">[</span><span class="nb">f64</span><span class="p">])</span> <span class="k">-&gt;</span> <span class="nb">f64</span> <span class="p">{</span>
    <span class="k">let</span> <span class="k">mut</span> <span class="n">dist</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">;</span>
    <span class="k">for</span> <span class="n">i</span> <span class="k">in</span> <span class="mi">0</span><span class="o">..</span><span class="n">x</span><span class="nf">.len</span><span class="p">()</span> <span class="p">{</span>
        <span class="n">dist</span> <span class="o">+=</span> <span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">])</span><span class="nf">.powi</span><span class="p">(</span><span class="mi">2</span><span class="p">);</span>
    <span class="p">}</span>
    <span class="n">dist</span><span class="nf">.sqrt</span><span class="p">()</span>
<span class="p">}</span>
</code></pre></div></div>

<p>But let‚Äôs think about the cost:</p>

<ul>
  <li>For every point, computing distances to all other points gives us <code class="language-plaintext highlighter-rouge">n * d</code>.</li>
  <li>Sorting distances gives <code class="language-plaintext highlighter-rouge">n log n</code>.</li>
</ul>

<p>So naive complexity is:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>O(n * d + n log n) -&gt; O(n log n)  (dominant term)
</code></pre></div></div>

<p>This is doable for small datasets but not optimal, especially for online or large-scale settings.</p>

<h2 id="the-interesting-part-of-knn">The Interesting Part of KNN</h2>

<p>To find nearest neighbors efficiently, we can partition the space into ‚Äúneighborhoods.‚Äù</p>

<ul>
  <li>One approach: k-d trees.</li>
  <li>Another approach: <strong>Locality Sensitive Hashing (LSH).</strong></li>
</ul>

<h3 id="what-is-lsh">What is LSH?</h3>

<p>LSH is a hashing technique that maps similar inputs to similar outputs. Here‚Äôs the intuition:</p>

<ol>
  <li>Choose a subset of dimensions.</li>
  <li>Add a small random perturbation to avoid boundary misalignment.</li>
  <li>Divide by a bucket width to assign items to discrete buckets.</li>
</ol>

<p>Mathematically:</p>

<div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>a: Standard Normal Vector ~ N(0, I)
b: Standard Uniform Scalar ~ U(0,1)
w: bucket width constant

hash := floor((a¬∑x - b) / w)
</code></pre></div></div>

<p>To improve robustness, we use multiple hash functions:</p>

<div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>H := (hash[0], hash[1], ..., hash[h])
</code></pre></div></div>

<h2 id="performance-analysis">Performance Analysis</h2>

<h3 id="inference">Inference</h3>

<p><strong>Inference steps:</strong></p>

<ol>
  <li>Compute the hash for each function in <code class="language-plaintext highlighter-rouge">H</code> for input <code class="language-plaintext highlighter-rouge">x</code>.</li>
  <li>Retrieve all points from the corresponding buckets.</li>
  <li>Remove duplicates, sort by Euclidean distance, and select top k.</li>
</ol>

<p><strong>Complexity:</strong></p>

<div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>let d = dimensions
let z = number of retrieved neighbors
let h = number of hash functions

O(d*h + h + z*log(z) + z*d) -&gt; O(z log z + C)
</code></pre></div></div>

<p>Compared to naive:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>O(n log n)
</code></pre></div></div>

<p>This approach drastically reduces inference cost for large datasets.</p>

<h3 id="insertion-parsing">Insertion (Parsing)</h3>

<p>For each new vector:</p>

<ol>
  <li>Compute each hash.</li>
  <li>Insert into the hashtable.</li>
</ol>

<p><strong>Complexity:</strong> <code class="language-plaintext highlighter-rouge">O(n * d * h)</code>
<strong>Memory:</strong> <code class="language-plaintext highlighter-rouge">O(n * h)</code></p>

<p>Each point is stored in a bucket for each hash function.</p>

<h2 id="probabilistic-analysis">Probabilistic Analysis</h2>

<p>How do we know LSH will find the correct neighbors?</p>

<p>Let:</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">n</code> = total elements</li>
  <li><code class="language-plaintext highlighter-rouge">b</code> = elements per bucket</li>
  <li><code class="language-plaintext highlighter-rouge">p</code> = probability a hash bucket contains the nearest neighbor</li>
  <li><code class="language-plaintext highlighter-rouge">q</code> = 1 - p</li>
</ul>

<p>With multiple hashes:</p>

<div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pr(1 hash) = p
pr(2 hashes) = 1 - q^2
pr(h hashes) = 1 - q^h
</code></pre></div></div>

<p>As <code class="language-plaintext highlighter-rouge">h ‚Üí ‚àû</code>, probability of finding a neighbor approaches 1.</p>

<p><strong>For LSH with vectors:</strong></p>

<div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>hash(x) = floor((a¬∑x - b)/w)
x* = x + Œµ

|a¬∑(x - x*) / w| = Œµ / w
</code></pre></div></div>

<p>The difference between perturbed vectors scales with <code class="language-plaintext highlighter-rouge">Œµ / w</code>. Probability that two vectors fall into the same bucket:</p>

<div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Pr(|a¬∑(x-y)/w| &lt; 1) = 2 * NormalCDF(w/d) - 1
</code></pre></div></div>

<p>This gives us a statistical bound for neighbor retrieval.</p>

<h2 id="wrapping-up">Wrapping Up</h2>

<p>We‚Äôve explored:</p>

<ul>
  <li>Basic KNN</li>
  <li>Efficient inference with LSH</li>
  <li>Complexity and memory considerations</li>
  <li>Probabilistic guarantees for correctness</li>
</ul>

<p>There are further optimizations in Rust, like using <code class="language-plaintext highlighter-rouge">Arc</code> to avoid cloning data, but those are beyond this post‚Äôs scope.</p>

<p>I hope this post shows how a simple algorithm can become fascinating once you dive into performance and probabilistic analysis. Implementing KNN from scratch with these techniques is both instructive and practical.</p>

<p>Happy coding!</p>

    </div>
  </div>

<div class="window">
    <div class="window-header">
      First Taste of Learnings <small>(2025-06-12)</small>
    </div>
    <div class="window-content">
      <p><a href="https://github.com/cyancirrus/async_feedboard">Rust Fully Asynchronous BlueSky-like study</a></p>

<h2 id="first-async-application-in-rust-and-first-fully-async-backend-ever">First Async Application in Rust (and first fully async backend ever!)</h2>

<p>Recently, I‚Äôve been trying to wrap my head around using async within Rust.<br />
I had some prior experience using async in Python, mainly to make non-blocking calls for embeddings of identified terms when they were independent.<br />
Although I had some background, I was still greatly intimidated ‚Äî many developers, who appeared far more talented, spoke about how difficult it was to understand Rust‚Äôs async model.</p>

<h3 id="dont-be-scared--jump-in">Don‚Äôt Be Scared ‚Äî <em>Jump In</em></h3>

<p>Surprisingly, transitioning my message-board-like or BlueSky-like app from a LeetCode solution into a fully async backend wasn‚Äôt terribly difficult.<br />
First, I worked through several async problems focused on using notifications and streaming. Then, without much further practice, I jumped in.</p>

<p>The API structure ‚Äî being mostly async ‚Äî was really about learning the tools:</p>
<ul>
  <li><strong>Axum</strong> for the API tree and server</li>
  <li><strong>Serde</strong> to handle JSON and the barrier between client and server</li>
  <li><strong>Tokio</strong> for the async runtime and its async-aware RwLock</li>
  <li><strong>std::sync::Arc</strong> for atomic reference counting so I could clone handles without cloning the actual data</li>
</ul>

<p>But my <em>favorite</em> package ‚Äî the standout for me‚Ä¶
‚Äì <strong>DashMap</strong> an amazing tool allowing you to manage interior mutability</p>

<h2 id="what-the-data-looked-like-on-the-backend">What the Data Looked Like on the Backend</h2>

<p>One of the main APIs I needed to port was <code class="language-plaintext highlighter-rouge">fn follow(...)</code>, which takes in a <code class="language-plaintext highlighter-rouge">followee_id</code> and a <code class="language-plaintext highlighter-rouge">follower_id</code>.<br />
I wanted users to safely write to their own portion of the data ‚Äî i.e., a user (the follower) clicks <em>follow</em> on another user (the followee), and we remember this information.</p>

<p>This was tricky to model. The user should only be able to modify their own data. It should also be fully async.</p>

<p>Originally, the data appeared in synchronous code as:</p>
<div class="language-rust highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">HashMap</span><span class="o">&lt;</span><span class="n">UserId</span><span class="p">,</span> <span class="n">HashSet</span><span class="o">&lt;</span><span class="n">UserId</span><span class="o">&gt;&gt;</span>
</code></pre></div></div>
<p>A mapping from user ID to the set of users they follow.<br />
I used a set to enable quick <code class="language-plaintext highlighter-rouge">unfollow</code> (O(1)) and to ensure no duplicate follows.</p>

<h3 id="first-iteration">First Iteration</h3>

<p>To model the problem, I initially reached for a <code class="language-plaintext highlighter-rouge">Mutex</code>.<br />
Mutexes were useful when solving async LeetCode problems, allowing mutation within async code ‚Äî so I started there.</p>
<div class="language-rust highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">Mutex</span><span class="o">&lt;</span><span class="n">HashSet</span><span class="o">&lt;</span><span class="n">UserId</span><span class="o">&gt;&gt;</span>
</code></pre></div></div>

<p>This worked, but it was blocking.<br />
It was synchronous code masquerading as async.<br />
Time to explore other structures beyond what I‚Äôd seen in my limited Rust async exposure.</p>

<h3 id="enter-rwlock-read-write-lock">Enter RwLock (Read-Write Lock)</h3>

<p>My API could naturally be partitioned into:</p>
<ul>
  <li><strong>Read actions</strong>: <code class="language-plaintext highlighter-rouge">NewsFeed</code></li>
  <li><strong>Write actions</strong>: <code class="language-plaintext highlighter-rouge">Follow</code>, <code class="language-plaintext highlighter-rouge">Publish</code>, <code class="language-plaintext highlighter-rouge">Unfollow</code></li>
</ul>

<p>This seemed like a natural fit for <code class="language-plaintext highlighter-rouge">RwLock</code>, so I implemented:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>RwLock&lt;HashMap&lt;UserId, RwLock&lt;HashSet&lt;UserId&gt;&gt;&gt;&gt;
</code></pre></div></div>

<p>There were only a few code changes: <code class="language-plaintext highlighter-rouge">.lock().await</code> became <code class="language-plaintext highlighter-rouge">.read().await</code> or <code class="language-plaintext highlighter-rouge">.write().await</code>. Overall, the changes were minimal.</p>

<p><code class="language-plaintext highlighter-rouge">RwLock</code> was a major improvement over <code class="language-plaintext highlighter-rouge">Mutex</code> ‚Äî while <code class="language-plaintext highlighter-rouge">Mutex</code> allows only a single user or thread to access the data at a time, <code class="language-plaintext highlighter-rouge">RwLock</code> allowed multiple readers in parallel.</p>

<p>‚Ä¶but the problem remained: most actions cause side effects (writes), and a single write on the outermost <code class="language-plaintext highlighter-rouge">RwLock</code> blocked the entire backend ‚Äî even reads!<br />
Multiple users writing to different locations ‚Äî they <em>should</em> be able to write independently.<br />
This separation had to be modelable. How could I drive that separation?</p>

<h3 id="dashmap-the-sleeper-wizzard">DashMap: The Sleeper Wizzard</h3>

<p>The problem seemed so simple: just enable read-write locking on the interior data.<br />
Enter <strong>DashMap</strong> ‚Äî like Gandalf cresting the hill at Helm‚Äôs Deep!</p>

<p>DashMap allows users to mutate their private data without needing explicit mutability, and it‚Äôs a near drop-in replacement for <code class="language-plaintext highlighter-rouge">HashMap</code>.</p>

<p>For example:</p>
<div class="language-rust highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">fn</span> <span class="nf">follow</span><span class="p">(</span><span class="o">&amp;</span><span class="k">mut</span> <span class="k">self</span><span class="p">,</span> <span class="n">followee_id</span><span class="p">:</span> <span class="n">UserId</span><span class="p">,</span> <span class="n">follower_id</span><span class="p">:</span> <span class="n">UserId</span><span class="p">)</span>
</code></pre></div></div>
<p>became:</p>
<div class="language-rust highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">fn</span> <span class="nf">follow</span><span class="p">(</span><span class="o">&amp;</span><span class="k">self</span><span class="p">,</span> <span class="n">followee_id</span><span class="p">:</span> <span class="n">UserId</span><span class="p">,</span> <span class="n">follower_id</span><span class="p">:</span> <span class="n">UserId</span><span class="p">)</span>
</code></pre></div></div>

<p>This helped clean up parts of Axum‚Äôs server model and the guarantees needed to build the API tree.<br />
DashMap enabled private mutation ‚Äî as long as you handled the interior structure correctly, e.g.:</p>
<div class="language-rust highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">DashMap</span><span class="o">&lt;</span><span class="n">UserId</span><span class="p">,</span> <span class="n">RwLock</span><span class="o">&lt;</span><span class="n">HashSet</span><span class="o">&lt;</span><span class="n">UserId</span><span class="o">&gt;&gt;&gt;</span>
</code></pre></div></div>

<p>This was exactly the model I was searching for.<br />
I <em>cannot</em> recommend the library enough if you‚Äôre facing a similar modeling problem where something feels like it <em>should</em> be possible.</p>

<h2 id="takeaway">Takeaway</h2>

<p>Not only is Rust async ‚Äî and its tooling ‚Äî becoming ever more mature and viable in production, but‚Ä¶</p>

<p><strong>Don‚Äôt be scared to jump in.</strong><br />
You‚Äôve already solved problems that felt impossible at the time. This is just another challenge.</p>

<p>When I saw my project handle 1,000 posts from 1,000 users and retrieve sorted newsfeeds for 10 users in 12.620865ms seconds on my 2018 machine, I was thrilled.</p>

<p>Programming isn‚Äôt just writing code for things you already know.<br />
Programming <em>is</em> solving new problems, exploring the unknown, and discovering better solutions.</p>

<p>Thanks so much for reading ‚Äî see you in the next post!</p>

    </div>
  </div>

<div class="window">
    <div class="window-header">
      First Taste of Learnings <small>(2025-06-11)</small>
    </div>
    <div class="window-content">
      <h2 id="welcome-to-my-blog-inaugural-post"><em>Welcome to my blog! Inaugural post!</em></h2>

<p>My name is Autumn, I‚Äôm a mixture of Data Scientist, Software Engineer, and Machine Learning Engineer.
I‚Äôm passionate about mathematics, statistics, performant computing, and low-level code.</p>

<p>I‚Äôve been exploring multiple projects focused on numerical computing and different strategies ‚Äî recently concentrating on matrix multiplication techniques, neural networks, and implementing various algorithms at a low level.</p>

<p>Additionally, I‚Äôve been brushing up on data structures and algorithms to design more performant systems. I have experience with databases, API development, predictive engines, model pipelines, and backend systems for applications.</p>

<p>Here, I hope to track my progress and provide a reference for others making the same journey.
Thanks so much for taking a look at my blog!</p>

    </div>
  </div>

<h2 id="current-studies">Current Studies</h2>
<ul>
  <li><a href="https://github.com/cyancirrus/matix">Pre-optimized scheduler</a></li>
  <li><a href="https://github.com/cyancirrus/stellar-math">Blas style math lib in rust</a></li>
  <li><a href="https://github.com/cyancirrus/neural-net">Neural net work-in-progress</a></li>
  <li><a href="https://github.com/cyancirrus/wordle">Fun wordle dynamic programming</a></li>
</ul>

  </div>
</body>
</html>

