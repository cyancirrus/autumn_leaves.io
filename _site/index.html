<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Autumn Leaves</title>
  <link rel="stylesheet" href="/style.css" />
</head>
<body>
  <div class="content-wrapper">
    <h1 id="-autumn-leaves">üçÇ Autumn Leaves</h1>

<h2 id="leafnotes-from-a-developer-on-a-journey-through-code-math-and-music">Leafnotes from a developer on a journey through code, math, and music</h2>

<div class="window">
    <div class="window-header">
      Computational Implemention of Randomized SVD <small>(2025-09-18)</small>
    </div>
    <div class="window-content">
      <p><a href="https://github.com/cyancirrus/stellar-math/blob/main/src/solver/randomized_svd.rs">Randomized Svd Algorithm</a>
<a href="https://github.com/cyancirrus/stellar-math/blob/main/src/decomposition/svd.rs">Golub Kahan Implementation</a>
<a href="https://github.com/cyancirrus/stellar-math/blob/main/src/decomposition/qr.rs">Optimized Qr Decomposition</a>
<a href="https://github.com/cyancirrus/stellar-math/blob/main/src/decomposition/givens.rs">Givens Implementation</a></p>

<p><a href="https://arxiv.org/pdf/0909.4061">Algorithm Source :: N. Halko, P. G. Martinsson, A. Tropp</a></p>

<h2 id="introduction">Introduction</h2>

<p>Hello, today i wish to dig into one of the basic cores of numerical, scientific and statistical computing - Singular Value Decomposition.</p>

<p>We‚Äôll be exploring some implementation details for SVD with some detailed focus on performance.</p>

<p>We will explore two main categories:</p>

<ul>
  <li>Optimizing some matrix expressions</li>
  <li>Using statistics for a lower rank approximation</li>
</ul>

<p>We‚Äôll be replicating the algorithm as detailed by within one of the siminal papers by Hako, Martinsson and Tropp ‚ÄúFinding Structure in Randomness‚Äù</p>

<p>Svd is everywhere from PCA to helping provide reliable large scale computes with intelligent updates without Huge Cost when data size grows far to large.</p>

<h2 id="what-is-svd">What is SVD</h2>

<p>Before we get into reduced rank SVD we should go over some basic ideas.</p>

<p><em>What is Singular Value Decomposition?</em></p>

<p>Singular Value Decomposition intuitively is a transform of a linear mapping such that we have three different main opperations</p>

<ul>
  <li>Input Rotation</li>
  <li>Scaling</li>
  <li>Output Rotation</li>
</ul>

<p>Okay that was hmmmm, a little more technical than I wanted it..</p>

<p>Consider the example: *We wish to model the chance that some will revist the hospital which we would have wished they would have been able to have a healthcare solution for already‚Äù</p>

<p>We‚Äôll consider that we have extensive demographic information</p>
<div class="language-md highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">-</span> age
<span class="p">-</span> gender
<span class="p">-</span> region
<span class="p">-</span> prescription-adherence
<span class="p">-</span> preventitive-care
<span class="p">-</span> previous year activity
<span class="p">-</span> bmi
... 20 other variables
</code></pre></div></div>
<p>However our input space in practice but although our input space is really quite large. As our input space is vast we can look across different factors and see how they jointly interact.
Imagine say that ‚Äúage, preventive-care, and prescription-adherence‚Äù dominate the main signal of our data. Svd will transform these multiple variables into a singular representation.</p>

<p>This new variable will contain the bulk of the joint signal or the latent (hidden or inferred) feature from this particular data. In fact the most dominant feature in this latent will be the first signular value and correspond to the first signular column. Each descending feature will then explain less and less signal from the data (due to definition and algorithmic implementation).</p>

<p>Continuing with this line of thought we can immediately see that if our data is noisy and we wish to impose some normalization we would be able to only consider say perhaps the top K for the eigenvalues and then transform our input data in order to represent only this signal.</p>

<p>The above idea is not only useful in Statistical Computing and DataScience, but also crucial for the performance of large scale matrices operations within ML and Engineering.</p>

<h2 id="implementantation-of-the-svd">Implementantation of the SVD</h2>

<h3 id="overview">Overview</h3>

<p>1) Detemerinistic SVD</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Bidiagonalize the matrix
- Golub Kahan via Householder of the column below the diagonal and the rows right of one element more than the diagonal
// note if you try to diagonalize directly with equal to diagonal this is just equal to Schurs and will only converge to a diagonal when the matrix is SPD

Chase the bulge down the bidiagonal and wait for convergence
- Givens Algorithm and extensions are the common methods

</code></pre></div></div>

<p>2) Randomized SVD</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Generate a random matrix Omega, Omega ~ Reduce rank, ie a subset of the features
O := Omega ~ Matrix[m, k]
Y = (AA')AO 
=&gt; Y ~ Matrix[m, k]

Normalize Y via QR
QR(Y) -&gt; (Q, R)
A' = Q*A

Run the deterministic SVD on the smaller matrix A'
</code></pre></div></div>

<h3 id="qr-algorithm-overview">QR Algorithm: Overview</h3>

<p>Lets first start to speak of this transformation as it‚Äôs used in the Randomized SVD algorithm but also it‚Äôs main basis for the Golub Kahan Algorithm (the algo which creates the bidiagonal, prior to bulge chasing which creates the SVD).</p>

<p>The most important part of QR is reconstruction ie when we seperate into 2 parts the orthogonal transformation and the scaling upper triangle matrix we receive the same input.</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>QR(A) = QR;
QR == A;
</code></pre></div></div>

<p>QR uses a product of householder transforms, which is essentially a reflection or rotation across an axis such that the resulting variable will be a multiple of e[i].</p>

<p>For the first dimension</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>H := Housholder
x := (1.0, 2.0, -3.14, 2.718)


// find the rotation such that for i := 1

// note each householder is a rank one outer product and is therefore symmetric
Q[i] := (I - Buu');

Q[i]x = sign(1.0) * (1.0, 0, 0, 0 ) * ||x|| 
Q[i]x = sign x[i] * e[i] ||x||
</code></pre></div></div>

<p>Recalling that rotations and reflections are orthogonal matrices, we then can create the following scheme</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Now we wish to reflect our data through this space so that everything zeros beneath the diagonal

Q[n] * Q[n-1] * .. * Q[1] A  = R

// noting from above that they are orthogonal
=&gt; Q'Q = I
=&gt; (Q[n] * .. * Q[1])' (Q[n] * .. * Q[1]) A  = (Q[n] * .. * Q[1])' R
A = (Q[n]*..*Q[1])' R
A = Q[1]'*..*Q[n]' R

// Remembering that they're symmetric from (I - Bvv')
A := Q[1]*..*Q[n] R 
A = QR
Q := Product Q[1]..Q[n];
</code></pre></div></div>

<p>Awesome! We just got the basic maths for Householder skipping some minor details - I recomment the book <em>Numerical Linear Algebra and Applications by Biswa Nath Datta</em>, it starts from zero and can take you up to reading papers and implementing algorithms in papers or algorithms for which you can derive invariants.</p>

<h3 id="qr-algorithm-compute">QR Algorithm: Compute</h3>

<p>Why did I just read that, you might be asking yourself dear reader, it‚Äôs because we can do some incredibly interesting computational optimizations with only the above information. Literally with the Above, we can reduce a matrix multiplication of unseen <em>A</em> against <em>Q</em> and have instead of <code class="language-plaintext highlighter-rouge">O(n^4)</code> for the calculation of QR we can have it be <code class="language-plaintext highlighter-rouge">O(n^3)</code>. This is incredibly pivotal and transforms the QR Algorithm from something ‚Äútheoretically interesting‚Äù to of practical importance <em>subnote we will be using the following optimizations for the calculation of Golub Kahan - bidiagonalization</em>.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>// Recalling both of the following
let Q := Product Q[1]..Q[n];
Q[i] := (I - Buu');

// Imagine we now wish to take the resultant of a matrix multiplied by Q ie
Qx = Output

// We can partition Q
let H[k] := Product 0..k Q[k]

this now infers the following

Q[k] = Q[k-1](I - B[k]v[k]v[k]')
Q[k] = Q[k-1] - B[k]Q[k-1]v[k]v[k]')
Q[k]

// Noting that this is just a Matrix * a vector * outer product
let w = B*Q[k-1]v[k];
=&gt;
Q' = Q - wv';
</code></pre></div></div>
<p>And note that this can be done for all matrices, the previous Q wasn‚Äôt special for the right hand optimization ie</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>QA
Q1*..*(I-buu')A

our recurrence for A:
let w := b*u'A;

A -= u'w'
</code></pre></div></div>
<p>and for left hand side</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>QA
A*(I-buu')*..* Qn

our recurrence for A:
let w := bAu;

A -= wu'
</code></pre></div></div>

<p>We have now managed to reudce the complexity of QR copute and computation significantly this has gotten a little abstract for myself so lets look at the underlying code</p>
<div class="language-rust highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">pub</span> <span class="k">fn</span> <span class="nf">qr_decompose</span><span class="p">(</span><span class="k">mut</span> <span class="n">x</span><span class="p">:</span> <span class="n">NdArray</span><span class="p">)</span> <span class="k">-&gt;</span> <span class="n">QrDecomposition</span> <span class="p">{</span>
    <span class="o">...</span>
        <span class="k">let</span> <span class="n">proj</span> <span class="o">=</span> <span class="nf">householder_params</span><span class="p">(</span><span class="n">column_vector</span><span class="p">);</span>
        <span class="c1">// x'A</span>
        <span class="k">for</span> <span class="n">j</span> <span class="k">in</span> <span class="n">o</span><span class="o">..</span><span class="n">cols</span> <span class="p">{</span>
            <span class="k">for</span> <span class="n">i</span> <span class="k">in</span> <span class="n">o</span><span class="o">..</span><span class="n">rows</span> <span class="p">{</span>
                <span class="n">w</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">+=</span> <span class="n">proj</span><span class="py">.vector</span><span class="p">[</span><span class="n">i</span> <span class="o">-</span> <span class="n">o</span><span class="p">]</span> <span class="o">*</span> <span class="n">x</span><span class="py">.data</span><span class="p">[</span><span class="n">i</span> <span class="o">*</span> <span class="n">cols</span> <span class="o">+</span> <span class="n">j</span><span class="p">];</span>
            <span class="p">}</span>
            <span class="n">w</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">*=</span> <span class="n">proj</span><span class="py">.beta</span><span class="p">;</span>
            <span class="k">for</span> <span class="n">i</span> <span class="k">in</span> <span class="n">o</span><span class="o">..</span><span class="n">rows</span> <span class="p">{</span>
                <span class="n">x</span><span class="py">.data</span><span class="p">[</span><span class="n">i</span> <span class="o">*</span> <span class="n">cols</span> <span class="o">+</span> <span class="n">j</span><span class="p">]</span> <span class="o">-=</span> <span class="n">proj</span><span class="py">.vector</span><span class="p">[</span><span class="n">i</span> <span class="o">-</span> <span class="n">o</span><span class="p">]</span> <span class="o">*</span> <span class="n">w</span><span class="p">[</span><span class="n">j</span><span class="p">];</span>
            <span class="p">}</span>
            <span class="n">w</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0_f32</span><span class="p">;</span>
        <span class="p">}</span>
        <span class="o">...</span>
    <span class="p">}</span>
    <span class="nn">QrDecomposition</span><span class="p">::</span><span class="nf">new</span><span class="p">(</span><span class="n">rows</span><span class="p">,</span> <span class="n">cols</span><span class="p">,</span> <span class="n">card</span><span class="p">,</span> <span class="n">projections</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
<span class="p">}</span>

</code></pre></div></div>
<p>The main part being highlighted here is the reduction in the computation cost underneath construction and a subnote that this is also available upon prediction very similar application of logic.</p>

<h3 id="qr-to-golub-kahan">QR to Golub Kahan</h3>

<p>There are several main differences between QR and Golub Kahan, the first being that QR just zeros below the diagonal for the construction of the upper triangular R matrix, and Golub Kahan chases the overhang ie row values to the right of the bidiagonal and column values blow the main diagonal - and that‚Äôs really <em>it</em>.</p>

<p>Golub Kahan is just the application of repeated zeroing procedures as we scan down the matrix. leaving the only non-zero entries within the 2 width band of the diagonals of a(i, i) and a(i, i+1).</p>

<p>Lets take a closer look of what that would look like</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>In order to zero'th the j'th column we would perform

Q[j]A = (I -buu')A[k]

and to zero'th the i'th row we would need to first transpose the matrix and then just zero below the intended target.

A[k+1] = Qi(A[k]')

lets continue for the next round, eliminating some of the indices for readability let C := 

Dropping the Q prefix for now as we're considering both Column and Row zeroing

c1 := Householder transform in order to zero column 1
r1 &lt;- Householder transform in order to zero for row 1 past the second entry

first step
gk' = c1 * A

zero beyond the row, which is equal to transpose and column zero
gk' = r1*(c1*A)'
gk' = r1 * A' * c1'

no lets do the column row just like in our Golub Kahan procedure
gk' = c2 * (r1 * A' * c1')'
gk' = c2 * c1 * A * r1'

recalling that they're symmetric and extending the pattern
qk := product column_zeroing[i] A row_zeroing[i];
</code></pre></div></div>

<p>So all we do is iterate back and forth between the column zeroing behavior and the row zeroing behavior until we arrive at our bidiagonal. The exact same optimization in QR also provided the life blood for the slightly more advanced implementation in Golub Kahan!</p>

<h2 id="statistical-optimization">Statistical Optimization</h2>

<p>Great so we‚Äôve had an in depth on how to optimize certain main core parts of this algorithm, and while all of this sounds pretty great, there‚Äôs nothing really <em>revolutionary</em>.</p>

<p>Lets now dig into the best part of this whole exploration - the Randomized SVD.
Lets consider the following example, say our matrix <code class="language-plaintext highlighter-rouge">source ~ 10_000 x 10_000</code> and we wish to compute a matrix multiplication with another <code class="language-plaintext highlighter-rouge">target ~ 10_000 x 10_000</code> matrix.</p>

<p>Naive implementation would have that our entire computational cost is <code class="language-plaintext highlighter-rouge">O(n^3) = 1e5^3 = 1e15</code> <em>One Trillion FLOPS!!!!</em>.
However consider we do Randomized with SVD because we know there is underlying signal within our data and we chose <code class="language-plaintext highlighter-rouge">k = 1_000</code>.</p>

<p>Entire Flops for truncated representations‚Äô calculation</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
let T := Target  ~ Matrix[10_000, 10_000];
// Truncated
let S := Source ~ Matrix[10_000, 10_000];
=&gt; O(SVD[k]*T) = O(n^2/k) 


Naive :: O(n^3)
SVD[l] :: O(n^2*k)

Computational Work Savings // we now do x% of work

Percent Work := O(n^2*k)/O(n^3)
**** Percent Work = n/k ****
</code></pre></div></div>

<p>For k sampled at 10% of the total dimensionality we only need to do 10% work which will dramatically reduce number of computations.
Incredibly important is we often don‚Äôt need such a rich representation of K, but one can imagine this compounding over numerical pipelines.
This is the magical part of SVD, is that it‚Äôs compounded affects throughout the pipeline for main workhouse matrix opperations.</p>

<h2 id="conclusion">Conclusion</h2>

<p>I hoped you had some fun exploring some of the optimizations within these algorithms. It‚Äôs amazing how we can find optimizations both within algebraic, statistical and computational structures.
I hope you saw something you could latch on to and feel free to explore the code-base, more and more unit tests are coming as the contracts and api is starting to stabalize.</p>

<p>Thanks so much for reading</p>


    </div>
  </div>

<div class="window">
    <div class="window-header">
      K-Nearest Neighbors from Scratch <small>(2025-09-18)</small>
    </div>
    <div class="window-content">
      <p><a href="https://github.com/cyancirrus/stellar-math/blob/main/src/learning/knn.rs">KNN Machine Learning Study</a></p>

<h2 id="introduction">Introduction</h2>

<p>Machine learning and AI are everywhere, so let‚Äôs dive into one of the foundational topics‚Äî<em>not deep learning this time!</em></p>

<p>I wanted to explore one of the most elegant and straightforward algorithms: K-Nearest Neighbors (KNN).</p>

<p>KNN is versatile: it can be used for both regression and classification. It even has connections to electronic engineering through Voronoi diagrams and is ubiquitous in machine learning.</p>

<h2 id="what-is-knn">What is KNN</h2>

<p>KNN has one central premise: <strong>data points close to each other in input space will produce similar outputs.</strong></p>

<ul>
  <li>For regression, we can average the outputs of the neighbors (either simple or weighted).</li>
  <li>For classification, we can either compute probabilities (via softmax) or return the majority vote for a hard classification.</li>
</ul>

<p>This method is extremely clear in its assumptions and behavior. The only requirement is that the data is somewhat continuous and smooth. It‚Äôs a great way to start implementing ML from scratch, without worrying about layers or activation functions, and it also allows for interesting engineering optimizations.</p>

<h2 id="knn-implementation-first-thoughts">KNN Implementation: First Thoughts</h2>

<p>Alright, let‚Äôs implement KNN. But how do we do it?</p>

<p><strong>Knowns:</strong></p>
<ul>
  <li>We need to find the top k closest neighbors for a point.</li>
  <li>We need a distance function.</li>
</ul>

<p>For the distance function, the natural choice is Euclidean distance (triangle distance):</p>

<div class="language-rust highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">fn</span> <span class="nf">distance</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="o">&amp;</span><span class="p">[</span><span class="nb">f64</span><span class="p">],</span> <span class="n">y</span><span class="p">:</span> <span class="o">&amp;</span><span class="p">[</span><span class="nb">f64</span><span class="p">])</span> <span class="k">-&gt;</span> <span class="nb">f64</span> <span class="p">{</span>
    <span class="k">let</span> <span class="k">mut</span> <span class="n">dist</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">;</span>
    <span class="k">for</span> <span class="n">i</span> <span class="k">in</span> <span class="mi">0</span><span class="o">..</span><span class="n">x</span><span class="nf">.len</span><span class="p">()</span> <span class="p">{</span>
        <span class="n">dist</span> <span class="o">+=</span> <span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">])</span><span class="nf">.powi</span><span class="p">(</span><span class="mi">2</span><span class="p">);</span>
    <span class="p">}</span>
    <span class="n">dist</span><span class="nf">.sqrt</span><span class="p">()</span>
<span class="p">}</span>
</code></pre></div></div>

<p>But let‚Äôs think about the cost:</p>

<ul>
  <li>For every point, computing distances to all other points gives us <code class="language-plaintext highlighter-rouge">n * d</code>.</li>
  <li>Sorting distances gives <code class="language-plaintext highlighter-rouge">n log n</code>.</li>
</ul>

<p>So naive complexity is:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>O(n * d + n log n) -&gt; O(n log n)  (dominant term)
</code></pre></div></div>

<p>This is doable for small datasets but not optimal, especially for online or large-scale settings.</p>

<h2 id="the-interesting-part-of-knn">The Interesting Part of KNN</h2>

<p>To find nearest neighbors efficiently, we can partition the space into ‚Äúneighborhoods.‚Äù</p>

<ul>
  <li>One approach: k-d trees.</li>
  <li>Another approach: <strong>Locality Sensitive Hashing (LSH).</strong></li>
</ul>

<h3 id="what-is-lsh">What is LSH?</h3>

<p>LSH is a hashing technique that maps similar inputs to similar outputs. Here‚Äôs the intuition:</p>

<ol>
  <li>Choose a subset of dimensions.</li>
  <li>Add a small random perturbation to avoid boundary misalignment.</li>
  <li>Divide by a bucket width to assign items to discrete buckets.</li>
</ol>

<p>Mathematically:</p>

<div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>a: Standard Normal Vector ~ N(0, I)
b: Standard Uniform Scalar ~ U(0,1)
w: bucket width constant

hash := floor((a¬∑x - b) / w)
</code></pre></div></div>

<p>To improve robustness, we use multiple hash functions:</p>

<div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>H := (hash[0], hash[1], ..., hash[h])
</code></pre></div></div>

<h2 id="performance-analysis">Performance Analysis</h2>

<h3 id="inference">Inference</h3>

<p><strong>Inference steps:</strong></p>

<ol>
  <li>Compute the hash for each function in <code class="language-plaintext highlighter-rouge">H</code> for input <code class="language-plaintext highlighter-rouge">x</code>.</li>
  <li>Retrieve all points from the corresponding buckets.</li>
  <li>Remove duplicates, sort by Euclidean distance, and select top k.</li>
</ol>

<p><strong>Complexity:</strong></p>

<div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>let d = dimensions
let z = number of retrieved neighbors
let h = number of hash functions

O(d*h + h + z*log(z) + z*d) -&gt; O(z log z + C)
</code></pre></div></div>

<p>Compared to naive:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>O(n log n)
</code></pre></div></div>

<p>This approach drastically reduces inference cost for large datasets.</p>

<h3 id="insertion-parsing">Insertion (Parsing)</h3>

<p>For each new vector:</p>

<ol>
  <li>Compute each hash.</li>
  <li>Insert into the hashtable.</li>
</ol>

<p><strong>Complexity:</strong> <code class="language-plaintext highlighter-rouge">O(n * d * h)</code>
<strong>Memory:</strong> <code class="language-plaintext highlighter-rouge">O(n * h)</code></p>

<p>Each point is stored in a bucket for each hash function.</p>

<h2 id="probabilistic-analysis">Probabilistic Analysis</h2>

<p>How do we know LSH will find the correct neighbors?</p>

<p>Let:</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">n</code> = total elements</li>
  <li><code class="language-plaintext highlighter-rouge">b</code> = elements per bucket</li>
  <li><code class="language-plaintext highlighter-rouge">p</code> = probability a hash bucket contains the nearest neighbor</li>
  <li><code class="language-plaintext highlighter-rouge">q</code> = 1 - p</li>
</ul>

<p>With multiple hashes:</p>

<div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pr(1 hash) = p
pr(2 hashes) = 1 - q^2
pr(h hashes) = 1 - q^h
</code></pre></div></div>

<p>As <code class="language-plaintext highlighter-rouge">h ‚Üí ‚àû</code>, probability of finding a neighbor approaches 1.</p>

<p><strong>For LSH with vectors:</strong></p>

<div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>hash(x) = floor((a¬∑x - b)/w)
x* = x + Œµ

|a¬∑(x - x*) / w| = Œµ / w
</code></pre></div></div>

<p>The difference between perturbed vectors scales with <code class="language-plaintext highlighter-rouge">Œµ / w</code>. Probability that two vectors fall into the same bucket:</p>

<div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Pr(|a¬∑(x-y)/w| &lt; 1) = 2 * NormalCDF(w/d) - 1
</code></pre></div></div>

<p>This gives us a statistical bound for neighbor retrieval.</p>

<h2 id="wrapping-up">Wrapping Up</h2>

<p>We‚Äôve explored:</p>

<ul>
  <li>Basic KNN</li>
  <li>Efficient inference with LSH</li>
  <li>Complexity and memory considerations</li>
  <li>Probabilistic guarantees for correctness</li>
</ul>

<p>There are further optimizations in Rust, like using <code class="language-plaintext highlighter-rouge">Arc</code> to avoid cloning data, but those are beyond this post‚Äôs scope.</p>

<p>I hope this post shows how a simple algorithm can become fascinating once you dive into performance and probabilistic analysis. Implementing KNN from scratch with these techniques is both instructive and practical.</p>

<p>Happy coding!</p>

    </div>
  </div>

<div class="window">
    <div class="window-header">
      First Taste of Learnings <small>(2025-06-12)</small>
    </div>
    <div class="window-content">
      <p><a href="https://github.com/cyancirrus/async_feedboard">Rust Fully Asynchronous BlueSky-like study</a></p>

<h2 id="first-async-application-in-rust-and-first-fully-async-backend-ever">First Async Application in Rust (and first fully async backend ever!)</h2>

<p>Recently, I‚Äôve been trying to wrap my head around using async within Rust.<br />
I had some prior experience using async in Python, mainly to make non-blocking calls for embeddings of identified terms when they were independent.<br />
Although I had some background, I was still greatly intimidated ‚Äî many developers, who appeared far more talented, spoke about how difficult it was to understand Rust‚Äôs async model.</p>

<h3 id="dont-be-scared--jump-in">Don‚Äôt Be Scared ‚Äî <em>Jump In</em></h3>

<p>Surprisingly, transitioning my message-board-like or BlueSky-like app from a LeetCode solution into a fully async backend wasn‚Äôt terribly difficult.<br />
First, I worked through several async problems focused on using notifications and streaming. Then, without much further practice, I jumped in.</p>

<p>The API structure ‚Äî being mostly async ‚Äî was really about learning the tools:</p>
<ul>
  <li><strong>Axum</strong> for the API tree and server</li>
  <li><strong>Serde</strong> to handle JSON and the barrier between client and server</li>
  <li><strong>Tokio</strong> for the async runtime and its async-aware RwLock</li>
  <li><strong>std::sync::Arc</strong> for atomic reference counting so I could clone handles without cloning the actual data</li>
</ul>

<p>But my <em>favorite</em> package ‚Äî the standout for me‚Ä¶
‚Äì <strong>DashMap</strong> an amazing tool allowing you to manage interior mutability</p>

<h2 id="what-the-data-looked-like-on-the-backend">What the Data Looked Like on the Backend</h2>

<p>One of the main APIs I needed to port was <code class="language-plaintext highlighter-rouge">fn follow(...)</code>, which takes in a <code class="language-plaintext highlighter-rouge">followee_id</code> and a <code class="language-plaintext highlighter-rouge">follower_id</code>.<br />
I wanted users to safely write to their own portion of the data ‚Äî i.e., a user (the follower) clicks <em>follow</em> on another user (the followee), and we remember this information.</p>

<p>This was tricky to model. The user should only be able to modify their own data. It should also be fully async.</p>

<p>Originally, the data appeared in synchronous code as:</p>
<div class="language-rust highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">HashMap</span><span class="o">&lt;</span><span class="n">UserId</span><span class="p">,</span> <span class="n">HashSet</span><span class="o">&lt;</span><span class="n">UserId</span><span class="o">&gt;&gt;</span>
</code></pre></div></div>
<p>A mapping from user ID to the set of users they follow.<br />
I used a set to enable quick <code class="language-plaintext highlighter-rouge">unfollow</code> (O(1)) and to ensure no duplicate follows.</p>

<h3 id="first-iteration">First Iteration</h3>

<p>To model the problem, I initially reached for a <code class="language-plaintext highlighter-rouge">Mutex</code>.<br />
Mutexes were useful when solving async LeetCode problems, allowing mutation within async code ‚Äî so I started there.</p>
<div class="language-rust highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">Mutex</span><span class="o">&lt;</span><span class="n">HashSet</span><span class="o">&lt;</span><span class="n">UserId</span><span class="o">&gt;&gt;</span>
</code></pre></div></div>

<p>This worked, but it was blocking.<br />
It was synchronous code masquerading as async.<br />
Time to explore other structures beyond what I‚Äôd seen in my limited Rust async exposure.</p>

<h3 id="enter-rwlock-read-write-lock">Enter RwLock (Read-Write Lock)</h3>

<p>My API could naturally be partitioned into:</p>
<ul>
  <li><strong>Read actions</strong>: <code class="language-plaintext highlighter-rouge">NewsFeed</code></li>
  <li><strong>Write actions</strong>: <code class="language-plaintext highlighter-rouge">Follow</code>, <code class="language-plaintext highlighter-rouge">Publish</code>, <code class="language-plaintext highlighter-rouge">Unfollow</code></li>
</ul>

<p>This seemed like a natural fit for <code class="language-plaintext highlighter-rouge">RwLock</code>, so I implemented:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>RwLock&lt;HashMap&lt;UserId, RwLock&lt;HashSet&lt;UserId&gt;&gt;&gt;&gt;
</code></pre></div></div>

<p>There were only a few code changes: <code class="language-plaintext highlighter-rouge">.lock().await</code> became <code class="language-plaintext highlighter-rouge">.read().await</code> or <code class="language-plaintext highlighter-rouge">.write().await</code>. Overall, the changes were minimal.</p>

<p><code class="language-plaintext highlighter-rouge">RwLock</code> was a major improvement over <code class="language-plaintext highlighter-rouge">Mutex</code> ‚Äî while <code class="language-plaintext highlighter-rouge">Mutex</code> allows only a single user or thread to access the data at a time, <code class="language-plaintext highlighter-rouge">RwLock</code> allowed multiple readers in parallel.</p>

<p>‚Ä¶but the problem remained: most actions cause side effects (writes), and a single write on the outermost <code class="language-plaintext highlighter-rouge">RwLock</code> blocked the entire backend ‚Äî even reads!<br />
Multiple users writing to different locations ‚Äî they <em>should</em> be able to write independently.<br />
This separation had to be modelable. How could I drive that separation?</p>

<h3 id="dashmap-the-sleeper-wizzard">DashMap: The Sleeper Wizzard</h3>

<p>The problem seemed so simple: just enable read-write locking on the interior data.<br />
Enter <strong>DashMap</strong> ‚Äî like Gandalf cresting the hill at Helm‚Äôs Deep!</p>

<p>DashMap allows users to mutate their private data without needing explicit mutability, and it‚Äôs a near drop-in replacement for <code class="language-plaintext highlighter-rouge">HashMap</code>.</p>

<p>For example:</p>
<div class="language-rust highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">fn</span> <span class="nf">follow</span><span class="p">(</span><span class="o">&amp;</span><span class="k">mut</span> <span class="k">self</span><span class="p">,</span> <span class="n">followee_id</span><span class="p">:</span> <span class="n">UserId</span><span class="p">,</span> <span class="n">follower_id</span><span class="p">:</span> <span class="n">UserId</span><span class="p">)</span>
</code></pre></div></div>
<p>became:</p>
<div class="language-rust highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">fn</span> <span class="nf">follow</span><span class="p">(</span><span class="o">&amp;</span><span class="k">self</span><span class="p">,</span> <span class="n">followee_id</span><span class="p">:</span> <span class="n">UserId</span><span class="p">,</span> <span class="n">follower_id</span><span class="p">:</span> <span class="n">UserId</span><span class="p">)</span>
</code></pre></div></div>

<p>This helped clean up parts of Axum‚Äôs server model and the guarantees needed to build the API tree.<br />
DashMap enabled private mutation ‚Äî as long as you handled the interior structure correctly, e.g.:</p>
<div class="language-rust highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">DashMap</span><span class="o">&lt;</span><span class="n">UserId</span><span class="p">,</span> <span class="n">RwLock</span><span class="o">&lt;</span><span class="n">HashSet</span><span class="o">&lt;</span><span class="n">UserId</span><span class="o">&gt;&gt;&gt;</span>
</code></pre></div></div>

<p>This was exactly the model I was searching for.<br />
I <em>cannot</em> recommend the library enough if you‚Äôre facing a similar modeling problem where something feels like it <em>should</em> be possible.</p>

<h2 id="takeaway">Takeaway</h2>

<p>Not only is Rust async ‚Äî and its tooling ‚Äî becoming ever more mature and viable in production, but‚Ä¶</p>

<p><strong>Don‚Äôt be scared to jump in.</strong><br />
You‚Äôve already solved problems that felt impossible at the time. This is just another challenge.</p>

<p>When I saw my project handle 1,000 posts from 1,000 users and retrieve sorted newsfeeds for 10 users in 12.620865ms seconds on my 2018 machine, I was thrilled.</p>

<p>Programming isn‚Äôt just writing code for things you already know.<br />
Programming <em>is</em> solving new problems, exploring the unknown, and discovering better solutions.</p>

<p>Thanks so much for reading ‚Äî see you in the next post!</p>

    </div>
  </div>

<div class="window">
    <div class="window-header">
      First Taste of Learnings <small>(2025-06-11)</small>
    </div>
    <div class="window-content">
      <h2 id="welcome-to-my-blog-inaugural-post"><em>Welcome to my blog! Inaugural post!</em></h2>

<p>My name is Autumn, I‚Äôm a mixture of Data Scientist, Software Engineer, and Machine Learning Engineer.
I‚Äôm passionate about mathematics, statistics, performant computing, and low-level code.</p>

<p>I‚Äôve been exploring multiple projects focused on numerical computing and different strategies ‚Äî recently concentrating on matrix multiplication techniques, neural networks, and implementing various algorithms at a low level.</p>

<p>Additionally, I‚Äôve been brushing up on data structures and algorithms to design more performant systems. I have experience with databases, API development, predictive engines, model pipelines, and backend systems for applications.</p>

<p>Here, I hope to track my progress and provide a reference for others making the same journey.
Thanks so much for taking a look at my blog!</p>

    </div>
  </div>

<h2 id="current-studies">Current Studies</h2>
<ul>
  <li><a href="https://github.com/cyancirrus/matix">Pre-optimized scheduler</a></li>
  <li><a href="https://github.com/cyancirrus/stellar-math">Blas style math lib in rust</a></li>
  <li><a href="https://github.com/cyancirrus/neural-net">Neural net work-in-progress</a></li>
  <li><a href="https://github.com/cyancirrus/wordle">Fun wordle dynamic programming</a></li>
</ul>

  </div>
</body>
</html>

