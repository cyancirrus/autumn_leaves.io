---
layout: post
title: "K-Nearest Neighbors from Scratch"
date: 2025-09-18
tags: [rust, machine-learning, backend, learning]
---

[KNN Machine Learning Study](https://github.com/cyancirrus/stellar-math/blob/main/src/learning/knn.rs)

## Introduction

Machine learning and AI are everywhere, so let's dive into one of the foundational topics—_not deep learning this time!_

I wanted to explore one of the most elegant and straightforward algorithms: K-Nearest Neighbors (KNN).  

KNN is versatile: it can be used for both regression and classification. It even has connections to electronic engineering through Voronoi diagrams and is ubiquitous in machine learning.

## What is KNN

KNN has one central premise: **data points close to each other in input space will produce similar outputs.**  

- For regression, we can average the outputs of the neighbors (either simple or weighted).  
- For classification, we can either compute probabilities (via softmax) or return the majority vote for a hard classification.

This method is extremely clear in its assumptions and behavior. The only requirement is that the data is somewhat continuous and smooth. It's a great way to start implementing ML from scratch, without worrying about layers or activation functions, and it also allows for interesting engineering optimizations.

## KNN Implementation: First Thoughts

Alright, let's implement KNN. But how do we do it?

**Knowns:**
- We need to find the top k closest neighbors for a point.  
- We need a distance function.  

For the distance function, the natural choice is Euclidean distance (triangle distance):

```rust
fn distance(x: &[f64], y: &[f64]) -> f64 {
    let mut dist = 0.0;
    for i in 0..x.len() {
        dist += (x[i] - y[i]).powi(2);
    }
    dist.sqrt()
}
```

But let's think about the cost:

* For every point, computing distances to all other points gives us `n * d`.
* Sorting distances gives `n log n`.

So naive complexity is:

```
O(n * d + n log n) -> O(n log n)  (dominant term)
```

This is doable for small datasets but not optimal, especially for online or large-scale settings.

## The Interesting Part of KNN

To find nearest neighbors efficiently, we can partition the space into "neighborhoods."

* One approach: k-d trees.
* Another approach: **Locality Sensitive Hashing (LSH).**

### What is LSH?

LSH is a hashing technique that maps similar inputs to similar outputs. Here's the intuition:

1. Choose a subset of dimensions.
2. Add a small random perturbation to avoid boundary misalignment.
3. Divide by a bucket width to assign items to discrete buckets.

Mathematically:

```text
a: Standard Normal Vector ~ N(0, I)
b: Standard Uniform Scalar ~ U(0,1)
w: bucket width constant

hash := floor((a·x - b) / w)
```

To improve robustness, we use multiple hash functions:

```text
H := (hash[0], hash[1], ..., hash[h])
```

## Performance Analysis

### Inference

**Inference steps:**

1. Compute the hash for each function in `H` for input `x`.
2. Retrieve all points from the corresponding buckets.
3. Remove duplicates, sort by Euclidean distance, and select top k.

**Complexity:**

```text
let d = dimensions
let z = number of retrieved neighbors
let h = number of hash functions

O(d*h + h + z*log(z) + z*d) -> O(z log z + C)
```

Compared to naive:

```
O(n log n)
```

This approach drastically reduces inference cost for large datasets.

### Insertion (Parsing)

For each new vector:

1. Compute each hash.
2. Insert into the hashtable.

**Complexity:** `O(n * d * h)`
**Memory:** `O(n * h)`

Each point is stored in a bucket for each hash function.

## Probabilistic Analysis

How do we know LSH will find the correct neighbors?

Let:

* `n` = total elements
* `b` = elements per bucket
* `p` = probability a hash bucket contains the nearest neighbor
* `q` = 1 - p

With multiple hashes:

```text
pr(1 hash) = p
pr(2 hashes) = 1 - q^2
pr(h hashes) = 1 - q^h
```

As `h → ∞`, probability of finding a neighbor approaches 1.

**For LSH with vectors:**

```text
hash(x) = floor((a·x - b)/w)
x* = x + ε

|a·(x - x*) / w| = ε / w
```

The difference between perturbed vectors scales with `ε / w`. Probability that two vectors fall into the same bucket:

```text
Pr(|a·(x-y)/w| < 1) = 2 * NormalCDF(w/d) - 1
```

This gives us a statistical bound for neighbor retrieval.

## Wrapping Up

We've explored:

* Basic KNN
* Efficient inference with LSH
* Complexity and memory considerations
* Probabilistic guarantees for correctness

There are further optimizations in Rust, like using `Arc` to avoid cloning data, but those are beyond this post's scope.

I hope this post shows how a simple algorithm can become fascinating once you dive into performance and probabilistic analysis. Implementing KNN from scratch with these techniques is both instructive and practical.

Happy coding!
