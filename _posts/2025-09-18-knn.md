---
layout: post
title: "K-Nearest Neighbors from Scratch"
date: 2025-06-12
tags: [rust, machine-learning, backend, learning]
---

[KNN Machine Learning Study](https://github.com/cyancirrus/stellar-math/blob/main/src/learning/knn.rs)

## Introduction

Machine Learning and AI it's absolutely everywhere, so lets dive into one of the most complex topics _deep learning...._
Nope! we're not going to be going into that today :).

I wanted to explore one of the most elegant and trivial algorithms.
One which can be used for either regression or classification - KNN.

KNN has connections in Electronic Engineering with Voronoi diagrams and is ubiquitous in machine learning.

## What is KNN

KNN has one central premise at the core "data within the small neighborhood around the point will produce similar measures".
Here the measure could be classification, it could be regression - but it's all the same assumption "similar input will produce similar output".

For regression we could linear average the output of the neighbors or we could perform some sort of weighted average.
For classification we could either retrieve a vector of probabilaties via a softmax or we could just return the majority vote for a hard classification.

This method is extremely clear in it's behavior as well as it's assumption, the only thing that it requires is that the data is continuous and somewhat smooth.
It's a great way to get started into implementing your on ML from scratch and there won't be layer/activation function headaches - plus there's some really great engineering and optimizations.

## KNN Implementation First Thoughts

Awesome, so we're going to implement the algorithm KNN... but how would we do such a thing?

Knowns:
- We need to find the top k closest neighbors for a point
- ...
- Profit?$$?

Okie this seems pretty clear, all we need is a distance function between two vectors, lets try to remember
- dot product // hmmm this is cosine similarity and it's signed not this one
- lets use the analogue of root((a - b)^2) // simple triangle distance!

Vector analogue for triangle distance
```
distance (x, y) :: {
 dist = 0;
 for i in 0..length {
  dist += (x[i] - y[i])^2
 }
 dist.root()
}
```

Great! we have our distance function. But lets take a step back and consider what we're doing
- If we do this for every point our inference cost would be
- Size of data * Size of dimension + Sort over this result

If I'm remembering my big O's correctly this would be 
```
O(n * d + n * log(n))
->
O(nlogn)
```

Which while this is doable this is far from optimal and wouldn't be able to be online, and for larger datasets this would be attrocious..

## The Interesting Part of KNN

Okay lets step back, we know that if we want closest neighbors we might be able to partition into "neighborhoods", perhaps we could use k-d trees or something...

or!!!!

We could use a function which maps similar vectors to similar buckets, like a range or a filter.
We could imagine we could take a random subset of the dimensions and then see if they're the same...
But then that's only a subset that could be lossy and we would get only one chance...

*Local Sensitivity Hashing*

What is Local Sensitivity Hashing (LSH)? LSH is a hash function which maps like inputs to like outputs and coherently concretizes our ideas above

- We choose a subset of dimensions
- We add in some amount of noise to perterb the data
- We then divide by a bucket width in order to push these items into a given bucket

It uses the subset properties of the dimensions in order to find similar neighbors..
But.. this is lossy?! What are we supposed to do there's no guarantee!

Here's the magic, if instead we only used one hash function, what happens if we used "n" if we used 3 or 10 or 50?
What would happen ... I'm getting ahead of myself here, before we go into the probability analysis of our guarantees (are there any? can you find a quick bound yourself?) lets close the thread on LSH.

We want to choose a subset of dimensions --> lets chose a random vector, we want the data to respect distances and to be more likely to not wildly distort the items
We want to ensure there aren't misalignments on a speicfic boundary --> lets chose a random number in order to slightly perterb the data so it changes whether it "falls left" or "falls right"
Bucket width.. -> This is just a constant in which we wish to roughly have N/w amount of data within each bucket


Hashing Function
```
a: Standard Normal Vector ~ Normal(0, I); // "I" is just the identity matrix and basically means for each vector component the variance = 1
b: Standard Uniform Vector ~ Uniform(0, 1); // A random float scalar between 0..1
w: Constant ~ Float; // A constant that will bucket the outputs into discrete sections


hash := floor[ (ax - b) / w ]
```

With this we now have similar numbers bucketed and then lets start to explore the final analytical theoretics.
Now lets simply combine with more than one hashing function so we have

```
H := (hash[0], hash[1], .., hash[h]); // h different hashses
```

## Performance Big O's


### Inference

Effect: We now have these hash functions which can filter the entire data into specific neighborhoods.

How does inference work?

1) For the input vector "x" compute the hash for each of the functions in `H`
2) Retrieve all results from the different buckets
3) Remove Duplicates / Sort by Euclidian Distance / Truncate to top k

Lets analyze the O for compute

1) hash functions ~ `#{ dimensions } * #{ number of hash functions }`
2) retrieval ~ `#{ number of hash functions }`
3) remove, sort, truncate ~ `#{ neighbors * log (neighbors) } + #{ neighbors } * #{ dimensions }`

*Compute*
Inference:
```
let dimensions := d
let neighbors := z
let hashes := h

inference(k, x) ~= O( d * h + h +  z * log(z) + z * d)
->
O(zlogz + C) 
```
Now our inference is O(zlogz) which incredibly more performant than our naive

*Compute*
```
O(nlog(n))
```
So we've truncated the number of points from N down to the filtered subset!
This will scale for large datasets, as long as we have a good partition function which boils down to a satisfying choice of "w" which guides bucket width we'll be golden.
In fact we can ensure that the data within each bucket remains relatively constant such that `O(zlogz)` is nearly invariant with respect to `n` although the number of hash functions will need to scale with regards to dimensions.

### Parsing

For parsing lets think through this, for each new vector

1) Compute each hash
2) Insert into the hashtable
->

*Compute*
```
O(n * d * h)
```

Next lets consider the memory of our solution

1) Each point will be stored in a bucket for each hash function
->

*Memory*
```
O( n * h)
```

## Probabalistic Analysis for Correctness

The solution sounds great and is amazing but.. how can we know that it works?
How can we know that it even converges or that we can limit our error?

To answer the above questions we'll need to delve into a little bit of statistical analysis.


First lets consider the easiest bound to derive, lets look at the most naive and see if we increase hash functions that we can limit error
```
let n = total number of elements in the data
let b = number of elements in a bucket
let p = probability that the hash bucket contains the data which is closest for knn
let q = probability that the neighbor is not in the bucket for a given data

// trivially
q = (1-p);

// naive estimate
p = b/w;

// now lets look at what happens as we increase the number of hash functions

// probability using one hash
pr = p;

// probabability using 2 hashes
pr = 1 - (probability not in first bucket) * probability not in second bucket)
pr = 1 - q^2

// lets express as function of n

pr(n) = 1 - q^n
```

Now we can step back and clearly see that the limit as `n -> inf` probability we'll find the data becomes one, a certainty.
This is great! This means that we can now start to place error bounds on what is our hashing and filtering, which is extremely important when it comes to real world environments.

Next lets consider a deeper analysis of LSH to see if we can refine our bounds
```
LSH :: floor[ (ax - b)/w ]

a ~ Normal(0, I)
b ~ Uniform(0, 1)


let x:Vector;
let x*:Vector;

// from initialization and definition
x* := x + e;

difference = (ax - b)/w - (ax* -b)/w;
= (ax - b - (ax* - b) )/w;
= (ax - ax*)/ w;
= a(x - x*)/ w;
= e / w;

// so the difference between a vector and it's perterbation will merely be proportional to e/w;

similarly for two vectors
let x:Vector;
let y:Vector;


=> difference = a(x - y)/w;

// next lets express the event that they are within the same bucket as a probability

Pr(|a(x-y)/w| < 1)
= Pr(|a(x-y)| < w)
= Pr(-w < a(x -y) < w)

// great now lets analyze the argument 'a(x-y)'

let d := (x - y)
Variance of a(x-y)

// first lets analyze as constants for intution)
Variance(cX) = E[(cX)^2] - E[cX]^2
= c^2 * E[X^2] - c^2 * E[X]^2
= c^2 Variance(X)

=> Variance(cX) = c^2 Variance(X)
-> Variance(ad) ~ d^2 * Variance(a)
-> Variance = d^2 I

// however we are within vectors so we need to use a bit more robust analysis

let u := mean 
Var(ad) = E[(a'd - u)(a'd - u)']
= E[a'dd'a - a'du' - ud'a + uu']

// noting that E[ad] == u and u ~ scalar
= E[a'dd'a] - [uu] 

// noting that mean a ~ 0
E[ad] ~ E[a]E[d] = 0 * _ = 0


=>
Var(ad) = E[a'dd'a] = E[d'aa'd] = d^2 * Var(a)
==> 
Variance of a(x-y) = d^2 * I



// fantastic now we have bounds and we simply need to find our goal probability
= Pr(-w < a(x -y) < w)
= Pr(-w < ad < w)

let z = ad, per above we knkow
ad ~ Normal(0, d^2I)

pr(-w < ad < w)
= 2 NormalCdf(w / d) - 1 // by symmetry

```

Fantastic! Now we have our probabilistic bounds!
The final question is great we have the probability for one point, how do we ensure or even use this information we don't want to run this analysis for the entire dataset, what do we do?

We can subsample the entire data
```
let n := entire data set
let sample := n / k


ie presume
n := 1000
s := 100

// if we were doing 10 nearest neighbors we would presume 1 of the neighbors to be within the set
// next we could find the above analysis for the probability for each of the neighbors ie

// Normal approx
2 NormalCdf(w / d) - 1 // by symmetry

// finally we could check for some small subset of those within the sample that our probabilities are tollerable
```

Another way to check would be to generate points with noise and see what the probabilities would be for those.
Ie determine a failure rate acceptable for your application and then just run through the points and see how these relate to the given probabilities


## Wrapping up

There's some more topics we could discuss with this exploration, for example the importance of using `Arc` which are reference counts of an object, in order to prevent cloning within Rust and much more, however this post's scope has already grown large.

Today I wanted to just show an optimization trick for KNN which I found personally fascinating - so much so it inspired me to implement the solution from scratch.
And the final thing I wished to show was the Computational and the Memory constraints, but perhaps most importantly the Probabalistic and Error bounds which are derivable through analysis.

Hope you had fun!
